{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Migration and sync of assets between prod and staging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently the production API is the one that has the latest updated data by the WRI team. \n",
    "This notebook copies assets from `production` to `staging` maintening the match between IDs. Optionally, it would be possible to copy assets back from `staging` to `production`. \n",
    "\n",
    "### Steps:\n",
    "1. upload/update assest to `production`\n",
    "2. make a copy of the assests from `production` to `staging` using this script\n",
    "3. synchronise the ids of the assets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "1. run the `Functions`.\n",
    "2. create a list with the assets urls to copy.\n",
    "3. `Processing` has the steps to carry out the migration. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions\n",
    "These are the functions we need to create and synchronise assets from `staging` to `production`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import requests as re\n",
    "import json\n",
    "from datetime import datetime\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "staging_server = \"https://staging-api.globalforestwatch.org\"\n",
    "prod_server = \"https://api.resourcewatch.org\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class bcolors:\n",
    "    HEADER = '\\033[95m'\n",
    "    OKBLUE = '\\033[94m'\n",
    "    OKCYAN = '\\033[96m'\n",
    "    OKGREEN = '\\033[92m'\n",
    "    WARNING = '\\033[93m'\n",
    "    FAIL = '\\033[91m'\n",
    "    ENDC = '\\033[0m'\n",
    "    BOLD = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auth(env='prod'):\n",
    "    serverUrl = {\n",
    "        'prod': prod_server,\n",
    "        'staging': staging_server\n",
    "    }\n",
    "    print(f'You are login into {bcolors.HEADER}{bcolors.BOLD}{env}{bcolors.ENDC}')\n",
    "    with re.Session() as s:\n",
    "        headers = {'Content-Type': 'application/json'}\n",
    "        payload = json.dumps({ 'email': f'{input(f\"Email: \")}',\n",
    "                               'password': f'{getpass.getpass(prompt=\"Password: \")}'})\n",
    "        response = s.post(f'{serverUrl[env]}/auth/login',  headers = headers,  data = payload)\n",
    "        response.raise_for_status()\n",
    "        print(f'{bcolors.OKGREEN}Successfully logged into {env}{bcolors.ENDC}')\n",
    "    return response.json().get('data').get('token')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are login into \u001b[95m\u001b[1mstaging\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Email:  alicia.arenzana@vizzuality.com\n",
      "Password:  ··········\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mSuccessfully logged into staging\u001b[0m\n",
      "You are login into \u001b[95m\u001b[1mprod\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Email:  alicia.arenzana@vizzuality.com\n",
      "Password:  ··········\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mSuccessfully logged into prod\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "token = {\n",
    "    'staging': auth('staging'),\n",
    "    'prod':auth('prod')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @TODO \n",
    "# * Migrate one day the body payloads to data model classes and refactor to classes following inheritance and recursive property copies\n",
    "# * Type function with Mypy\n",
    "# * Add proper method descriptions\n",
    "# * Refactor methods to reuse more code\n",
    "#from typing import List\n",
    "#from pydantic import BaseModel, parse_obj_as\n",
    "# class DatasetModel(BaseModel):\n",
    "\n",
    "# class LayerModel(BaseModel):\n",
    "\n",
    "# class widgetModel(BaseModel):\n",
    "\n",
    "# class metadataModel(BaseModel):\n",
    "     \n",
    "# class vocabularyModel(BaseModel):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setTokenHeader(env, token=token):\n",
    "    '''\n",
    "    set up the token\n",
    "    '''\n",
    "    return {'Authorization':f'Bearer {token[env]}', \n",
    "            'Content-Type': 'application/json'}\n",
    "\n",
    "def logResponseErrors(status_code, response = None, url = None, body = None):\n",
    "    '''\n",
    "    log errors in http calls\n",
    "    '''\n",
    "    if status_code !=200:\n",
    "        logging.error('response: ')\n",
    "        logging.error(response)\n",
    "        logging.error(response.text) if response else None\n",
    "        logging.error(response.json()) if response else None\n",
    "        logging.error('url: ')\n",
    "        logging.error(url) if url else None\n",
    "        logging.error('body: ')\n",
    "        logging.error(json.dumps(body)) if body else None\n",
    "    \n",
    "\n",
    "def getAssets(url, payload):\n",
    "    '''\n",
    "    Get asset operation\n",
    "    '''\n",
    "    response = re.get(url, payload)\n",
    "    \n",
    "    logResponseErrors(response.status_code, response, url, payload)\n",
    "    \n",
    "    response.raise_for_status()\n",
    "    \n",
    "    return response.json()\n",
    "\n",
    "def deleteAssets(url, headers):\n",
    "    '''\n",
    "    delete asset operation\n",
    "    '''\n",
    "    response = re.delete(url, headers = headers)\n",
    "    \n",
    "    logResponseErrors(response.status_code, response, url)\n",
    "    \n",
    "    response.raise_for_status()\n",
    "    \n",
    "    return response.status_code\n",
    "\n",
    "def postAssets(url, body, headers, payloads = None):\n",
    "    '''\n",
    "    create asset operation\n",
    "    '''    \n",
    "    response = re.post(url, params = payloads, data=json.dumps(body), headers = headers)\n",
    "    \n",
    "    logResponseErrors(response.status_code, response, url, body)\n",
    "    \n",
    "    response.raise_for_status()\n",
    "    \n",
    "    return response.json()\n",
    "\n",
    "def updateAssets(url, body, headers):\n",
    "    '''\n",
    "    patch asset operation\n",
    "    '''\n",
    "    response = re.patch(url, data=json.dumps(body), headers = headers)\n",
    "    \n",
    "    logResponseErrors(response.status_code, response, url, body)\n",
    "    \n",
    "    response.raise_for_status()\n",
    "    \n",
    "    return response.json()\n",
    "\n",
    "def copyAssetBody(asset, excludeList=['createdAt', 'updatedAt','clonedHost', 'errorMessage', 'taskId', 'status', 'sources',\n",
    "                                      'userId', 'slug', 'dataset', 'layer', 'widget', 'metadata', 'vocabulary']):\n",
    "    '''\n",
    "    Copy a body dict to a new dict excluding some keys or not defined values.\n",
    "    '''\n",
    "    response = {}\n",
    "    response.update(asset)\n",
    "    \n",
    "    for key, value in asset.items():\n",
    "        if (key in excludeList or value is None or (type(value) == dict and len(value) == 0) ):\n",
    "            response.pop(key, None)\n",
    "    \n",
    "    if 'provider' in response.keys() and response['provider'] =='cartodb':\n",
    "        response.pop('tableName', None)\n",
    "    \n",
    "    return response\n",
    "\n",
    "def upsert(conditon = False):\n",
    "    '''\n",
    "    Return an update/post operation base on a condition\n",
    "    '''\n",
    "    if conditon:\n",
    "        return updateAssets\n",
    "    else:\n",
    "        return postAssets\n",
    "    \n",
    "def recreateDataset(dataset, toEnv = 'prod', destinationDatasetId = None):\n",
    "    '''\n",
    "    Copy the dataset from one env to the other\n",
    "    '''\n",
    "    \n",
    "    serverUrl = {\n",
    "        'prod': prod_server,\n",
    "        'staging': staging_server\n",
    "    }\n",
    "    if dataset.get('type')!='dataset':\n",
    "        return None\n",
    "    \n",
    "    url = f'{serverUrl[toEnv]}/v1/dataset'\n",
    "    \n",
    "    if destinationDatasetId:\n",
    "        url = f'{url}/{destinationDatasetId}' \n",
    "        \n",
    "    body = {'dataset': copyAssetBody(dataset.get('attributes'))}\n",
    "    \n",
    "    logger.debug(body)\n",
    "    \n",
    "    headers = setTokenHeader(toEnv)\n",
    "    \n",
    "    response = upsert(destinationDatasetId)\n",
    "    \n",
    "    logger.debug(response)\n",
    "    if destinationDatasetId:       \n",
    "        return response(url, body['dataset'], headers)\n",
    "    else:\n",
    "        return response(url, body, headers)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "def recreateLayer(datasetId, layer, toEnv = 'prod', destinationLayerId = None):\n",
    "    '''\n",
    "    Copy the layer from one env to the other\n",
    "    '''\n",
    "    \n",
    "    serverUrl = {\n",
    "        'prod': prod_server,\n",
    "        'staging': staging_server\n",
    "    }\n",
    "    if layer.get('type')!='layer':\n",
    "        return None\n",
    "    \n",
    "    headers = setTokenHeader(toEnv)\n",
    "    url = f'{serverUrl[toEnv]}/v1/dataset/{datasetId}/layer'\n",
    "    \n",
    "    if destinationLayerId:\n",
    "        url = f'{url}/{destinationLayerId}'\n",
    "\n",
    "    body = copyAssetBody(layer.get('attributes'))\n",
    "    \n",
    "    response = upsert(destinationLayerId)\n",
    "    \n",
    "    return response(url, body, headers)\n",
    "\n",
    "def recreateWidget(datasetId, widget, toEnv = 'prod', destinationWidgetId = None):\n",
    "    '''\n",
    "    Copy the widget from one env to the other\n",
    "    '''\n",
    "    \n",
    "    serverUrl = {\n",
    "        'prod': prod_server,\n",
    "        'staging': staging_server\n",
    "    }\n",
    "    \n",
    "    if widget.get('type')!='widget':\n",
    "        return None\n",
    "    \n",
    "    headers = setTokenHeader(toEnv)\n",
    "    url = f'{serverUrl[toEnv]}/v1/dataset/{datasetId}/widget'\n",
    "    \n",
    "    if destinationWidgetId:\n",
    "        url = f'{url}/{destinationWidgetId}'\n",
    "    \n",
    "    body = copyAssetBody(widget.get('attributes'))\n",
    "    \n",
    "    response = upsert(destinationWidgetId)\n",
    "    \n",
    "    return response(url, body, headers)\n",
    "\n",
    "def recreateMetadata(datasetId, metadata, layerId=None, widgetId=None, toEnv = 'prod'):\n",
    "    '''\n",
    "    Copy the metadata from one env to the other\n",
    "    '''\n",
    "    \n",
    "    serverUrl = {\n",
    "        'prod': prod_server,\n",
    "        'staging': staging_server\n",
    "    }\n",
    "    headers = setTokenHeader(toEnv)\n",
    "    \n",
    "    if metadata.get('type')!='metadata':\n",
    "        return None\n",
    "    if layerId and widgetId:\n",
    "        raise Exception(\"layerId and widgetId not allowed at the same time\")\n",
    "    elif layerId:\n",
    "        url = f'{serverUrl[toEnv]}/v1/dataset/{datasetId}/layer/{layerId}/metadata'\n",
    "    elif widgetId:\n",
    "        url = f'{serverUrl[toEnv]}/v1/dataset/{datasetId}/widget/{widgetId}/metadata'\n",
    "    else:\n",
    "        url = f'{serverUrl[toEnv]}/v1/dataset/{datasetId}/metadata'\n",
    "    \n",
    "    body = copyAssetBody(metadata.get('attributes'))\n",
    "    \n",
    "    try:\n",
    "        response = upsert()\n",
    "        return response(url, body, headers)\n",
    "    except Exception as e:\n",
    "        print('Post operation was not succesfull, trying to update instead')\n",
    "        response = upsert(True)\n",
    "        return response(url, body, headers)\n",
    "        pass\n",
    "\n",
    "def recreateVocabulary(datasetId, vocabulary, toEnv = 'prod'):\n",
    "    '''\n",
    "    Copy the vocabulary from one env to the other\n",
    "    '''\n",
    "    \n",
    "    serverUrl = {\n",
    "        'prod': prod_server,\n",
    "        'staging': staging_server\n",
    "    }\n",
    "    \n",
    "    if vocabulary.get('type')!='vocabulary':\n",
    "        return None\n",
    "    \n",
    "    headers = setTokenHeader(toEnv)\n",
    "    \n",
    "    url = f\"{serverUrl[toEnv]}/v1/dataset/{datasetId}/vocabulary/{vocabulary['attributes']['name']}\"\n",
    "    body = {\n",
    "        'application': vocabulary['attributes'].get('application'),\n",
    "        'tags': vocabulary['attributes'].get('tags')\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = upsert()\n",
    "        return response(url, body, headers)\n",
    "    except Exception as e:\n",
    "        print(f'{bcolors.WARNING}Post operation was not succesfull, trying to update instead{bcolors.ENDC}')\n",
    "        response = upsert(True)\n",
    "        return response(url, body, headers)\n",
    "        pass\n",
    "\n",
    "def getAssetList(fromEnv = 'prod', datasetList=None):\n",
    "    '''\n",
    "    Gets a list of assets from the selected env or from the constrained dataset list\n",
    "    '''\n",
    "    serverUrl = {\n",
    "        'prod': prod_server,\n",
    "        'staging': staging_server\n",
    "    }\n",
    "    headers = setTokenHeader(fromEnv)\n",
    "    url = f'{serverUrl[fromEnv]}/v1/dataset'\n",
    "    payload={\n",
    "        'application':'rw',\n",
    "        'status':'saved',\n",
    "        'includes':'widget,layer,vocabulary,metadata',\n",
    "        'page[size]':1613982331640\n",
    "    }\n",
    "    if datasetList:\n",
    "        url = f'{serverUrl[fromEnv]}/v1/dataset/find-by-ids'\n",
    "        body = {\n",
    "            'ids': datasetList\n",
    "        }\n",
    "        return postAssets(url, body, headers, payload)\n",
    "    else:\n",
    "        return getAssets(url, payload)\n",
    "    \n",
    "def backupAssets(env = 'prod', datasetList = None):\n",
    "    '''\n",
    "    save a backup of production data just in case we need to recreate it again\n",
    "    '''\n",
    "    data = getAssetList(env, datasetList)\n",
    "    \n",
    "\n",
    "    with open(f'RW_{Env}_backup_{datetime.now().strftime(\"%Y%m%d-%H%M%S\")}.json', 'w') as outfile:\n",
    "        json.dump(data, outfile)\n",
    "\n",
    "def deleteDataFrom(env='staging', datasetList = None):\n",
    "    '''\n",
    "    Deletes all assets from an env.\n",
    "    '''\n",
    "    serverUrl = {\n",
    "        'prod': prod_server,\n",
    "        'staging': staging_server\n",
    "    }\n",
    "    userConfirmation = input(f'{bcolors.WARNING}Are you sure you want to delete \\\n",
    "        {str(datasetList)  if datasetList else \"everything\" } in {env}:{bcolors.ENDC} \\\n",
    "        Y/n') or \"N\"\n",
    "    if userConfirmation == 'Y':\n",
    "        headers = setTokenHeader(env)\n",
    "        data = getAssetList(env, datasetList)\n",
    "        \n",
    "        for dataset in data['data']:\n",
    "            #@TODO: this needs to be reworked a bit\n",
    "            try:\n",
    "                logger.info(f\"deleting {serverUrl[env]}/v1/dataset/{dataset['id']}... \")\n",
    "                status = deleteAssets(f\"{serverUrl[env]}/v1/dataset/{dataset['id']}\", headers)\n",
    "                    \n",
    "            except re.exceptions.HTTPError as err:\n",
    "                logger.error(err)\n",
    "                pass\n",
    "    else:\n",
    "        print('nothing was deleted')\n",
    "\n",
    "def assetIdToBeSync(sync, syncList, assetToSync, fromEnv, toEnv):\n",
    "    '''\n",
    "    controls the asset id to be sync\n",
    "    '''\n",
    "    if sync:\n",
    "        return next((asset.get(f'{toEnv}Id') for asset in syncList \\\n",
    "                     if (asset.get('type') == assetToSync.get('type') \\\n",
    "                         and asset.get(f'{fromEnv}Id') == assetToSync.get('id'))), \n",
    "                    False)\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def copyAssets(assetList, sync=False, fromEnv='prod', toEnv='staging'):\n",
    "    '''\n",
    "    Creates a new copy or syncs the assets that we set up in the fromEnv into the destination Env \n",
    "    '''\n",
    "    if fromEnv == toEnv:\n",
    "        raise NameError(f'fromEnv:{fromEnv} and toEnv:{toEnv} cannot be the same')\n",
    "        \n",
    "    if not assetList or len(assetList) == 0:\n",
    "        raise IndexError(f'asset list is empty or not defined')\n",
    "        \n",
    "    \n",
    "    dataAssets = []    \n",
    "    \n",
    "    if sync:\n",
    "        newDatasetList = [asset[f'{fromEnv}Id'] for asset in assetList if asset['type'] == 'dataset']\n",
    "        dataAssets = getAssetList(fromEnv, newDatasetList)\n",
    "\n",
    "    else:   \n",
    "        dataAssets = getAssetList(fromEnv, assetList)\n",
    "    \n",
    "    try:\n",
    "        print(f'{bcolors.OKBLUE}Preparing to {\"sync\" if sync else \"copy\"} from {fromEnv} to {toEnv}...{bcolors.ENDC}')\n",
    "        resources = []\n",
    "        \n",
    "        # @TODO:\n",
    "        # Improve loop performance with multiprocessing\n",
    "        # move loops into reusable function based on type\n",
    "        # For sync only path updated data\n",
    "        \n",
    "        for dataset in dataAssets['data']:\n",
    "            toDatasetId = assetIdToBeSync(sync, assetList, dataset, fromEnv, toEnv)\n",
    "            if toDatasetId:\n",
    "                logger.info(f'sync [{fromEnv}]dataset: {dataset.get(\"id\")}')\n",
    "                logger.info(f'with [{toEnv}]dataset: {toDatasetId}')\n",
    "            newDataset = recreateDataset(dataset, toEnv, toDatasetId)\n",
    "\n",
    "            resources.append({\n",
    "                'type': 'dataset',\n",
    "                f'{fromEnv}Id':dataset.get('id'),\n",
    "                f'{toEnv}Id': newDataset['data'].get('id')\n",
    "            })\n",
    "\n",
    "            for vocabulary in dataset['attributes'].get('vocabulary'):\n",
    "                newVocabulary = recreateVocabulary(newDataset['data'].get('id'), vocabulary, toEnv)\n",
    "                \n",
    "                resources.append({\n",
    "                'type': 'vocabulary',\n",
    "                f'{fromEnv}Id':vocabulary.get('id'),\n",
    "                f'{toEnv}Id': newVocabulary['data']\n",
    "            })\n",
    "\n",
    "            for layer in dataset['attributes'].get('layer'):\n",
    "                toLayerId = assetIdToBeSync(sync, assetList, layer, fromEnv, toEnv)\n",
    "                if toLayerId:\n",
    "                    logger.info(f'sync [{fromEnv}]layer: {layer.get(\"id\")}')\n",
    "                    logger.info(f'with [{toEnv}]dlayer: {toLayerId}')\n",
    "                \n",
    "                newLayer = recreateLayer(newDataset['data'].get('id'), layer, toEnv, toLayerId)\n",
    "                \n",
    "                resources.append({\n",
    "                'type': 'layer',\n",
    "                f'{fromEnv}Id':layer.get('id'),\n",
    "                f'{toEnv}Id': newLayer['data'].get('id')\n",
    "            })\n",
    "\n",
    "            for widget in dataset['attributes'].get('widget'):\n",
    "                \n",
    "                toWidgetId = assetIdToBeSync(sync, assetList, widget, fromEnv, toEnv)\n",
    "                if toWidgetId:\n",
    "                    logger.info(f'sync [{fromEnv}]widget: {widget.get(\"id\")}')\n",
    "                    logger.info(f'with [{toEnv}]widget: {toWidgetId}')\n",
    "                    \n",
    "                newWidget = recreateWidget(newDataset['data'].get('id'), widget, toEnv, toWidgetId)\n",
    "                \n",
    "                resources.append({\n",
    "                'type': 'widget',\n",
    "                f'{fromEnv}Id':widget.get('id'),\n",
    "                f'{toEnv}Id': newWidget['data'].get('id')\n",
    "            })\n",
    "\n",
    "            for metadata in dataset['attributes'].get('metadata'):\n",
    "                \n",
    "                newMetadata = recreateMetadata(newDataset['data'].get('id'), metadata, toEnv=toEnv)\n",
    "                \n",
    "                resources.append({\n",
    "                'type': 'metadata',\n",
    "                f'{fromEnv}Id':metadata.get('id'),\n",
    "                f'{toEnv}Id': newMetadata['data']\n",
    "            })\n",
    "    except NameError or IndexError as e:\n",
    "        logger.error(e)\n",
    "        raise e\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    print(f'{bcolors.OKGREEN}{\"sync\" if sync else \"copy\"} process finished{bcolors.ENDC}')\n",
    "    \n",
    "    if not sync and len(resources) > 0:\n",
    "        filename = f'RW_prod_staging_match_{datetime.now().strftime(\"%Y%m%d-%H%M%S\")}.json'\n",
    "        print(f'creating sync file with name: {filename}')\n",
    "        with open(filename, 'w') as outfile:\n",
    "            json.dump(resources, outfile)\n",
    "        \n",
    "        return filename\n",
    "    \n",
    "    \n",
    "        \n",
    "def syncAssets(syncList, fromEnv='prod', toEnv='staging'):\n",
    "    '''\n",
    "    Allows sync of Assets\n",
    "    '''\n",
    "    \n",
    "    return copyAssets(syncList, True, fromEnv, toEnv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing\n",
    "## Get list of assets that we want to modify or sync"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List of assets:\n",
    "\n",
    "* `datasetsProd` will contain the id of the assets in productioon that need to be migrated to `staging`. We need to make sure that this list is in sync with the document we have shared with the assets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For testing purposes\n",
    "Dummy assests to create `datasetsProd`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': {'id': 'cfaf9dbd-a6e9-4bd5-b1ca-685639c34ba2',\n",
       "  'type': 'dataset',\n",
       "  'attributes': {'name': 'This is a test',\n",
       "   'slug': 'This-is-a-test_10',\n",
       "   'type': None,\n",
       "   'subtitle': None,\n",
       "   'application': ['rw'],\n",
       "   'dataPath': None,\n",
       "   'attributesPath': None,\n",
       "   'connectorType': 'rest',\n",
       "   'provider': 'cartodb',\n",
       "   'userId': '57a0aa1071e394dd32ffe137',\n",
       "   'connectorUrl': 'https://wri-rw.carto.com/api/v2/sql?q=select * from air_temo_anomalies',\n",
       "   'sources': [],\n",
       "   'tableName': 'air_temo_anomalies',\n",
       "   'status': 'pending',\n",
       "   'published': False,\n",
       "   'overwrite': False,\n",
       "   'mainDateField': None,\n",
       "   'env': 'production',\n",
       "   'geoInfo': False,\n",
       "   'protected': False,\n",
       "   'legend': {'date': [],\n",
       "    'region': [],\n",
       "    'country': [],\n",
       "    'nested': [],\n",
       "    'integer': [],\n",
       "    'short': [],\n",
       "    'byte': [],\n",
       "    'double': [],\n",
       "    'float': [],\n",
       "    'half_float': [],\n",
       "    'scaled_float': [],\n",
       "    'boolean': [],\n",
       "    'binary': [],\n",
       "    'text': [],\n",
       "    'keyword': []},\n",
       "   'clonedHost': {},\n",
       "   'errorMessage': None,\n",
       "   'taskId': None,\n",
       "   'createdAt': '2021-06-07T07:55:42.893Z',\n",
       "   'updatedAt': '2021-06-07T07:55:42.893Z',\n",
       "   'dataLastUpdated': None,\n",
       "   'widgetRelevantProps': [],\n",
       "   'layerRelevantProps': []}}}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dummy data to test the notebook: creation of a dummy dataset with a layer in production.\n",
    "toEnv = 'prod'\n",
    "serverUrl = {\n",
    "        'prod': prod_server,\n",
    "        'staging': staging_server\n",
    "    }\n",
    "headers = setTokenHeader(toEnv)\n",
    "urlDataset = f'{serverUrl[toEnv]}/v1/dataset'\n",
    "bodyDataset = {'dataset':{\n",
    "    'application': ['rw'],\n",
    "    'name': 'This is a test',\n",
    "    'connectorType': 'rest',\n",
    "    'provider': 'cartodb',\n",
    "    'published': False,\n",
    "    'overwrite': False,\n",
    "    'protected':False,\n",
    "    'env': 'production',\n",
    "    'connectorUrl': \"https://wri-rw.carto.com/api/v2/sql?q=select * from air_temo_anomalies\"\n",
    "    }\n",
    "}\n",
    "\n",
    "responseDataset = postAssets(urlDataset, bodyDataset, headers)\n",
    "responseDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': {'id': '91357283-ef05-4516-bb87-c48abb992c9c',\n",
       "  'type': 'layer',\n",
       "  'attributes': {'name': 'test-121',\n",
       "   'slug': 'test-121_3',\n",
       "   'dataset': 'cfaf9dbd-a6e9-4bd5-b1ca-685639c34ba2',\n",
       "   'application': ['rw'],\n",
       "   'iso': [],\n",
       "   'provider': 'cartodb',\n",
       "   'userId': '57a0aa1071e394dd32ffe137',\n",
       "   'default': True,\n",
       "   'protected': False,\n",
       "   'published': False,\n",
       "   'env': 'production',\n",
       "   'layerConfig': {'body': {}},\n",
       "   'legendConfig': {},\n",
       "   'interactionConfig': {},\n",
       "   'applicationConfig': {},\n",
       "   'staticImageConfig': {},\n",
       "   'createdAt': '2021-06-07T07:55:44.029Z',\n",
       "   'updatedAt': '2021-06-07T07:55:44.029Z'}}}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urlLayer = f'{urlDataset}/{responseDataset[\"data\"].get(\"id\")}/layer'\n",
    "bodyLayer = {\n",
    "        'application': ['rw'],\n",
    "        'name': 'test-121',\n",
    "        'provider': 'cartodb',\n",
    "        'default': True,\n",
    "        'published': False,\n",
    "        'env': 'production',\n",
    "        'layerConfig': {\n",
    "            \"body\": {}\n",
    "            },\n",
    "        'legendConfig': {},\n",
    "        'interactionConfig': {},\n",
    "        'applicationConfig': {}\n",
    "    }\n",
    "responseLayer = postAssets(urlLayer, bodyLayer, headers)\n",
    "responseLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': {'id': 'd34cf886-f44c-46a8-81ce-7eef945f12e7',\n",
       "  'type': 'widget',\n",
       "  'attributes': {'name': 'test-121',\n",
       "   'dataset': '9f4c0539-ecfe-4c67-9929-aa2f552d6f73',\n",
       "   'slug': 'test-121_2',\n",
       "   'userId': '57a0aa1071e394dd32ffe137',\n",
       "   'application': ['rw'],\n",
       "   'verified': False,\n",
       "   'default': True,\n",
       "   'protected': False,\n",
       "   'defaultEditableWidget': False,\n",
       "   'published': False,\n",
       "   'freeze': False,\n",
       "   'env': 'production',\n",
       "   'widgetConfig': {'body': {}},\n",
       "   'template': False,\n",
       "   'createdAt': '2021-06-07T07:38:57.400Z',\n",
       "   'updatedAt': '2021-06-07T07:38:57.400Z'}}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urlWidget = f'{urlDataset}/{responseDataset[\"data\"].get(\"id\")}/widget'\n",
    "bodyWidget = {\n",
    "        'application': ['rw'],\n",
    "        'name': 'test-121',\n",
    "        'default': True,\n",
    "        'published': False,\n",
    "        'env': 'production',\n",
    "        'widgetConfig': {\n",
    "            \"body\": {}\n",
    "            }\n",
    "    }\n",
    "responseWidget = postAssets(urlWidget, bodyWidget, headers)\n",
    "responseWidget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': [{'id': 'knowledge_graph',\n",
       "   'type': 'vocabulary',\n",
       "   'attributes': {'tags': ['geospatial'],\n",
       "    'name': 'knowledge_graph',\n",
       "    'application': 'rw'}}]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urlVocabulary = f'{urlDataset}/{responseDataset[\"data\"].get(\"id\")}/vocabulary/knowledge_graph'\n",
    "bodyVocabulary = {\n",
    "        'application': 'rw',\n",
    "        'tags':[\"geospatial\"]\n",
    "    }\n",
    "responseVocabulary = postAssets(urlVocabulary, bodyVocabulary, headers)\n",
    "responseVocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List of assets:\n",
    "\n",
    "* we need to make sure that this list is in sync with the document we have shared with the assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['9f4c0539-ecfe-4c67-9929-aa2f552d6f73']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# in the future we can automate this listing based on the doc using the google sheet api both for writing and reading from\n",
    "# providing a sample of the list by printing it\n",
    "datasetsProd = [responseDataset['data']['id']]\n",
    "datasetsProd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backup Data in both environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#backupAssets('prod')\n",
    "#backupAssets('staging')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only do this if you want to clean data in staging. \n",
    "* You will need to be logged in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#deleteDataFrom()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy resources from production to staging. \n",
    "The running time will depend on the size of the asset.   \n",
    "Running this cell is only needed to create new assets from `production` to `staging`.\n",
    "A json file is created with a unique name in local. The json files contains for each assest:\n",
    "- type: this can be a \"layer\", a \"dataset\", a \"widget\", \"vocabulary\", \"metadata\"\n",
    "- prodId: the id of the item in `production`\n",
    "- stagingId: the id of the item in `staging`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94mPreparing to copy from prod to staging...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:None\n",
      "INFO:root:None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mcopy process finished\u001b[0m\n",
      "creating sync file with name: RW_prod_staging_match_20210607-074853.json\n"
     ]
    }
   ],
   "source": [
    "syncFile = copyAssets(datasetsProd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open sync list of assets, match items with list and update them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:sync [prod]dataset: 9f4c0539-ecfe-4c67-9929-aa2f552d6f73\n",
      "INFO:root:with [staging]dataset: 4ff49ded-a2d1-4b49-a218-b3bc6d2b8b30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94mPreparing to sync from prod to staging...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:response: \n",
      "ERROR:root:<Response [400]>\n",
      "ERROR:root:url: \n",
      "ERROR:root:https://staging-api.globalforestwatch.org/v1/dataset/4ff49ded-a2d1-4b49-a218-b3bc6d2b8b30/vocabulary/knowledge_graph\n",
      "ERROR:root:body: \n",
      "ERROR:root:{\"application\": \"rw\", \"tags\": [\"geospatial\"]}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mPost operation was not succesfull, trying to update instead\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:sync [prod]layer: 0061b933-744c-4a67-b8c2-fb5ae3ae0fb3\n",
      "INFO:root:with [staging]dlayer: b73894dc-e50c-4c75-a8aa-079b66f8c67f\n",
      "INFO:root:sync [prod]widget: d34cf886-f44c-46a8-81ce-7eef945f12e7\n",
      "INFO:root:with [staging]widget: 7e6e1764-ec13-4382-8544-33fcb1ee050d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92msync process finished\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# use the printed json filename in the previous cell\n",
    "with open(syncFile) as json_file:\n",
    "    syncList = json.load(json_file)\n",
    "\n",
    "syncAssets(syncList, fromEnv='prod', toEnv='staging')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\u001b[93mAre you sure you want to delete         ['cfaf9dbd-a6e9-4bd5-b1ca-685639c34ba2'] in prod:\u001b[0m         Y/n Y\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:deleting https://api.resourcewatch.org/v1/dataset/cfaf9dbd-a6e9-4bd5-b1ca-685639c34ba2... \n"
     ]
    }
   ],
   "source": [
    "# delete testing datasets from both envs after testing:\n",
    "deleteDataFrom('prod', [responseDataset['data']['id']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\u001b[93mAre you sure you want to delete         ['4ff49ded-a2d1-4b49-a218-b3bc6d2b8b30'] in staging:\u001b[0m         Y/n Y\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:deleting https://staging-api.globalforestwatch.org/v1/dataset/4ff49ded-a2d1-4b49-a218-b3bc6d2b8b30... \n"
     ]
    }
   ],
   "source": [
    "deleteDataFrom('staging', [syncList[0]['stagingId']])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
