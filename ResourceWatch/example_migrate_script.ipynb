{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Migration and sync of assets between prod and staging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently the production API is the one that has the latest updated data by the WRI team. \n",
    "This notebook copies assets from `production` to `staging` maintening the match between IDs. Optionally, it would be possible to copy assets back from `staging` to `production`. \n",
    "\n",
    "### Steps:\n",
    "1. upload/update assest to `production`\n",
    "2. make a copy of the assests from `production` to `staging` using this script\n",
    "3. synchronise the ids of the assets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "1. run the `Functions`.\n",
    "2. create a list with the assets urls to copy.\n",
    "3. `Processing` has the steps to carry out the migration. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions\n",
    "These are the functions we need to create and synchronise assets from `staging` to `production`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import requests as re\n",
    "import json\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import time\n",
    "import os\n",
    "import dictdiffer\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "staging_server = \"https://staging-api.resourcewatch.org\"\n",
    "prod_server = \"https://api.resourcewatch.org\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class bcolors:\n",
    "    HEADER = '\\033[95m'\n",
    "    OKBLUE = '\\033[94m'\n",
    "    OKCYAN = '\\033[96m'\n",
    "    OKGREEN = '\\033[92m'\n",
    "    WARNING = '\\033[93m'\n",
    "    FAIL = '\\033[91m'\n",
    "    ENDC = '\\033[0m'\n",
    "    BOLD = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auth(env='prod'):\n",
    "    serverUrl = {\n",
    "        'prod': prod_server,\n",
    "        'staging': staging_server\n",
    "    }\n",
    "    print(f'You are login into {bcolors.HEADER}{bcolors.BOLD}{env}{bcolors.ENDC}')\n",
    "    with re.Session() as s:\n",
    "        headers = {'Content-Type': 'application/json'}\n",
    "        payload = json.dumps({ 'email': f'{input(f\"Email: \")}',\n",
    "                               'password': f'{getpass.getpass(prompt=\"Password: \")}'})\n",
    "        response = s.post(f'{serverUrl[env]}/auth/login',  headers = headers,  data = payload)\n",
    "        response.raise_for_status()\n",
    "        print(f'{bcolors.OKGREEN}Successfully logged into {env}{bcolors.ENDC}')\n",
    "    return response.json().get('data').get('token')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are login into \u001b[95m\u001b[1mstaging\u001b[0m\n",
      "Email: alicia.arenzana@vizzuality.com\n",
      "Password: ········\n",
      "\u001b[92mSuccessfully logged into staging\u001b[0m\n",
      "You are login into \u001b[95m\u001b[1mprod\u001b[0m\n",
      "Email: alicia.arenzana@vizzuality.com\n",
      "Password: ········\n",
      "\u001b[92mSuccessfully logged into prod\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "token = {\n",
    "    'staging': auth('staging'),\n",
    "    'prod':auth('prod')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @TODO \n",
    "# * Migrate one day the body payloads to data model classes and refactor to classes following inheritance and recursive property copies\n",
    "# * Type function with Mypy\n",
    "# * Add proper method descriptions\n",
    "# * Refactor methods to reuse more code\n",
    "# * https://findwork.dev/blog/advanced-usage-python-requests-timeouts-retries-hooks/ \n",
    "# retries and calls should be taking into account retry and backof factor from Request\n",
    "#from typing import List\n",
    "#from pydantic import BaseModel, parse_obj_as\n",
    "# class DatasetModel(BaseModel):\n",
    "\n",
    "# class LayerModel(BaseModel):\n",
    "\n",
    "# class widgetModel(BaseModel):\n",
    "\n",
    "# class metadataModel(BaseModel):\n",
    "     \n",
    "# class vocabularyModel(BaseModel):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setTokenHeader(env, token=token):\n",
    "    '''\n",
    "    set up the token\n",
    "    '''\n",
    "    return {'Authorization':f'Bearer {token[env]}', \n",
    "            'Content-Type': 'application/json'}\n",
    "\n",
    "def logResponseErrors(status_code, response = None, url = None, body = None):\n",
    "    '''\n",
    "    log errors in http calls\n",
    "    '''\n",
    "    if status_code !=200:\n",
    "        logging.error('response: ')\n",
    "        logging.error(response)\n",
    "        logging.error(response.text) if response else None\n",
    "        logging.error(response.json()) if response else None\n",
    "        logging.error('url: ')\n",
    "        logging.error(url) if url else None\n",
    "        logging.error('body: ')\n",
    "        logging.error(json.dumps(body)) if body else None\n",
    "    \n",
    "\n",
    "def getAssets(url, payload=None):\n",
    "    '''\n",
    "    Get asset operation\n",
    "    '''\n",
    "    response = re.get(url, payload)\n",
    "\n",
    "    try_num = 1\n",
    "    while response.status_code == 504 and try_num <= 3:\n",
    "        time.sleep(30)\n",
    "        response = re.get(url, payload)\n",
    "        try_num += 1\n",
    "\n",
    "    logResponseErrors(response.status_code, response, url, payload)\n",
    "    \n",
    "    response.raise_for_status()\n",
    "    \n",
    "    return response.json()\n",
    "\n",
    "def deleteAssets(url, headers):\n",
    "    '''\n",
    "    delete asset operation\n",
    "    '''\n",
    "    response = re.delete(url, headers = headers)\n",
    "    \n",
    "    try_num = 1\n",
    "    while response.status_code == 504 and try_num <= 3:\n",
    "        time.sleep(30)\n",
    "        response = re.delete(url, headers = headers)\n",
    "        try_num += 1\n",
    "\n",
    "    logResponseErrors(response.status_code, response, url)\n",
    "    \n",
    "    response.raise_for_status()\n",
    "    \n",
    "    return response.status_code\n",
    "\n",
    "def postAssets(url, body, headers, payloads = None):\n",
    "    '''\n",
    "    create asset operation\n",
    "    '''    \n",
    "    response = re.post(url, params = payloads, data=json.dumps(body), headers = headers)\n",
    "    \n",
    "    try_num = 1\n",
    "    while response.status_code == 504 and try_num <= 3:\n",
    "        time.sleep(30)\n",
    "        response = re.post(url, params = payloads, data=json.dumps(body), headers = headers)\n",
    "        try_num += 1\n",
    "\n",
    "    logResponseErrors(response.status_code, response, url, body)\n",
    "    \n",
    "    response.raise_for_status()\n",
    "    \n",
    "    return response.json()\n",
    "\n",
    "def updateAssets(url, body, headers):\n",
    "    '''\n",
    "    patch asset operation\n",
    "    '''\n",
    "    response = re.patch(url, data=json.dumps(body), headers = headers)\n",
    "    \n",
    "    try_num = 1\n",
    "    while response.status_code == 504 and try_num <= 3:\n",
    "        time.sleep(30)\n",
    "        response = re.patch(url, data=json.dumps(body), headers = headers)\n",
    "        try_num += 1\n",
    "\n",
    "    logResponseErrors(response.status_code, response, url, body)\n",
    "    \n",
    "    response.raise_for_status()\n",
    "    \n",
    "    return response.json()\n",
    "\n",
    "def copyAssetBody(asset, excludeList=['createdAt', 'updatedAt','clonedHost', 'errorMessage', 'taskId', 'status', 'sources',\n",
    "                                      'userId', 'slug', 'dataset', 'layer', 'widget', 'metadata', 'vocabulary']):\n",
    "    '''\n",
    "    Copy a body dict to a new dict excluding some keys or not defined values.\n",
    "    '''\n",
    "    response = {}\n",
    "    response.update(asset)\n",
    "    \n",
    "    for key, value in asset.items():\n",
    "        if (key in excludeList or value is None or (type(value) == dict and len(value) == 0) ):\n",
    "            response.pop(key, None)\n",
    "    \n",
    "    if 'provider' in response.keys() and response['provider'] =='cartodb':\n",
    "        response.pop('tableName', None)\n",
    "    \n",
    "    return response\n",
    "\n",
    "def upsert(conditon = False):\n",
    "    '''\n",
    "    Return an update/post operation base on a condition\n",
    "    '''\n",
    "    if conditon:\n",
    "        return updateAssets\n",
    "    else:\n",
    "        return postAssets\n",
    "    \n",
    "def recreateDataset(dataset, toEnv = 'staging', destinationDatasetId = None):\n",
    "    '''\n",
    "    Copy the dataset from one env to the other\n",
    "    '''\n",
    "    \n",
    "    serverUrl = {\n",
    "        'prod': prod_server,\n",
    "        'staging': staging_server\n",
    "    }\n",
    "    if dataset.get('type')!='dataset':\n",
    "        return None\n",
    "    \n",
    "    url = f'{serverUrl[toEnv]}/v1/dataset'\n",
    "    \n",
    "    if destinationDatasetId:\n",
    "        url = f'{url}/{destinationDatasetId}' \n",
    "        \n",
    "    body = {'dataset': copyAssetBody(dataset.get('attributes'))}\n",
    "    \n",
    "    logger.debug(body)\n",
    "    \n",
    "    headers = setTokenHeader(toEnv)\n",
    "    \n",
    "    response = upsert(destinationDatasetId)\n",
    "    \n",
    "    logger.debug(response)\n",
    "    if destinationDatasetId:       \n",
    "        return response(url, body['dataset'], headers)\n",
    "    else:\n",
    "        return response(url, body, headers)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "def recreateLayer(datasetId, layer, toEnv = 'staging', destinationLayerId = None):\n",
    "    '''\n",
    "    Copy the layer from one env to the other\n",
    "    '''\n",
    "    \n",
    "    serverUrl = {\n",
    "        'prod': prod_server,\n",
    "        'staging': staging_server\n",
    "    }\n",
    "    if layer.get('type')!='layer':\n",
    "        return None\n",
    "    \n",
    "    headers = setTokenHeader(toEnv)\n",
    "    url = f'{serverUrl[toEnv]}/v1/dataset/{datasetId}/layer'\n",
    "    \n",
    "    if destinationLayerId:\n",
    "        url = f'{url}/{destinationLayerId}'\n",
    "\n",
    "    body = copyAssetBody(layer.get('attributes'))\n",
    "    \n",
    "    response = upsert(destinationLayerId)\n",
    "    \n",
    "    return response(url, body, headers)\n",
    "\n",
    "def recreateWidget(datasetId, widget, toEnv = 'staging', destinationWidgetId = None):\n",
    "    '''\n",
    "    Copy the widget from one env to the other\n",
    "    '''\n",
    "    \n",
    "    serverUrl = {\n",
    "        'prod': prod_server,\n",
    "        'staging': staging_server\n",
    "    }\n",
    "    \n",
    "    if widget.get('type')!='widget':\n",
    "        return None\n",
    "    \n",
    "    headers = setTokenHeader(toEnv)\n",
    "    url = f'{serverUrl[toEnv]}/v1/dataset/{datasetId}/widget'\n",
    "    \n",
    "    if destinationWidgetId:\n",
    "        url = f'{url}/{destinationWidgetId}'\n",
    "    \n",
    "    body = copyAssetBody(widget.get('attributes'))\n",
    "    \n",
    "    response = upsert(destinationWidgetId)\n",
    "    \n",
    "    return response(url, body, headers)\n",
    "\n",
    "def getSubAssetMetadata(datasetId, layerId=None, widgetId=None, fromEnv = 'prod'):\n",
    "    '''\n",
    "    Get metadata that is not given back from main call\n",
    "    '''\n",
    "    serverUrl = {\n",
    "        'prod': prod_server,\n",
    "        'staging': staging_server\n",
    "    }\n",
    "    \n",
    "    if (layerId and widgetId) or (not layerId and not widgetId):\n",
    "        raise Exception(\"layerId and widgetId not allowed at the same time\")\n",
    "    elif layerId:\n",
    "        url = f'{serverUrl[fromEnv]}/v1/dataset/{datasetId}/layer/{layerId}/metadata'\n",
    "    elif widgetId:\n",
    "        url = f'{serverUrl[fromEnv]}/v1/dataset/{datasetId}/widget/{widgetId}/metadata'\n",
    "        \n",
    "    try:\n",
    "        return getAssets(url)\n",
    "    except Exception as e:\n",
    "        logger.info('Get operation was not successfull')\n",
    "        logger.error(f'{e}')\n",
    "        return None\n",
    "        pass\n",
    "    \n",
    "\n",
    "def recreateMetadata(datasetId, metadata, layerId=None, widgetId=None, toEnv = 'prod'):\n",
    "    '''\n",
    "    Copy the metadata from one env to the other\n",
    "    '''\n",
    "    \n",
    "    serverUrl = {\n",
    "        'prod': prod_server,\n",
    "        'staging': staging_server\n",
    "    }\n",
    "    headers = setTokenHeader(toEnv)\n",
    "    \n",
    "    if metadata.get('type')!='metadata':\n",
    "        return None\n",
    "    if layerId and widgetId:\n",
    "        raise Exception(\"layerId and widgetId not allowed at the same time\")\n",
    "    elif layerId:\n",
    "        url = f'{serverUrl[toEnv]}/v1/dataset/{datasetId}/layer/{layerId}/metadata'\n",
    "    elif widgetId:\n",
    "        url = f'{serverUrl[toEnv]}/v1/dataset/{datasetId}/widget/{widgetId}/metadata'\n",
    "    else:\n",
    "        url = f'{serverUrl[toEnv]}/v1/dataset/{datasetId}/metadata'\n",
    "    \n",
    "    body = copyAssetBody(metadata.get('attributes'))\n",
    "    \n",
    "    try:\n",
    "        response = upsert()\n",
    "        return response(url, body, headers)\n",
    "    except Exception as e:\n",
    "        print(f'{bcolors.WARNING}Post operation was not succesfull, trying to update instead{bcolors.ENDC}')\n",
    "        response = upsert(True)\n",
    "        return response(url, body, headers)\n",
    "        pass\n",
    "\n",
    "def recreateVocabulary(datasetId, vocabulary, toEnv = 'prod'):\n",
    "    '''\n",
    "    Copy the vocabulary from one env to the other\n",
    "    '''\n",
    "    \n",
    "    serverUrl = {\n",
    "        'prod': prod_server,\n",
    "        'staging': staging_server\n",
    "    }\n",
    "    \n",
    "    if vocabulary.get('type')!='vocabulary':\n",
    "        return None\n",
    "    \n",
    "    headers = setTokenHeader(toEnv)\n",
    "    \n",
    "    url = f\"{serverUrl[toEnv]}/v1/dataset/{datasetId}/vocabulary/{vocabulary['attributes']['name']}\"\n",
    "    body = {\n",
    "        'application': vocabulary['attributes'].get('application'),\n",
    "        'tags': vocabulary['attributes'].get('tags')\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = upsert()\n",
    "        return response(url, body, headers)\n",
    "    except Exception as e:\n",
    "        print(f'{bcolors.WARNING}Post operation was not succesfull, trying to update instead{bcolors.ENDC}')\n",
    "        response = upsert(True)\n",
    "        return response(url, body, headers)\n",
    "        pass\n",
    "\n",
    "def getAssetList(fromEnv = 'prod', datasetList=None):\n",
    "    '''\n",
    "    Gets a list of assets from the selected env or from the constrained dataset list\n",
    "    '''\n",
    "    serverUrl = {\n",
    "        'prod': prod_server,\n",
    "        'staging': staging_server\n",
    "    }\n",
    "    headers = setTokenHeader(fromEnv)\n",
    "    url = f'{serverUrl[fromEnv]}/v1/dataset'\n",
    "    payload={\n",
    "        'application':'rw',\n",
    "        'status':'saved',\n",
    "        'includes':'widget,layer,vocabulary,metadata',\n",
    "        'page[size]':1613982331640\n",
    "    }\n",
    "    if datasetList:\n",
    "        url = f'{serverUrl[fromEnv]}/v1/dataset/find-by-ids'\n",
    "        body = {\n",
    "            'ids': datasetList\n",
    "        }\n",
    "        return postAssets(url, body, headers, payload)\n",
    "    else:\n",
    "        return getAssets(url, payload)\n",
    "    \n",
    "def backupAssets(env = 'prod', datasetList = None):\n",
    "    '''\n",
    "    save a backup of production data just in case we need to recreate it again\n",
    "    '''\n",
    "    data = getAssetList(env, datasetList)\n",
    "    \n",
    "\n",
    "    with open(f'RW_{env}_backup_{datetime.now().strftime(\"%Y%m%d-%H%M%S\")}.json', 'w') as outfile:\n",
    "        json.dump(data, outfile)\n",
    "\n",
    "def deleteDataFrom(env='staging', datasetList = None):\n",
    "    '''\n",
    "    Deletes all assets from an env.\n",
    "    '''\n",
    "    serverUrl = {\n",
    "        'prod': prod_server,\n",
    "        'staging': staging_server\n",
    "    }\n",
    "    userConfirmation = input(f'{bcolors.WARNING}Are you sure you want to delete \\\n",
    "        {str(datasetList)  if datasetList else \"everything\" } in {env}:{bcolors.ENDC} \\\n",
    "        Y/n') or \"N\"\n",
    "    if userConfirmation == 'Y':\n",
    "        headers = setTokenHeader(env)\n",
    "        data = getAssetList(env, datasetList)\n",
    "        \n",
    "        for dataset in data['data']:\n",
    "            #@TODO: this needs to be reworked a bit\n",
    "            try:\n",
    "                logger.info(f\"deleting {serverUrl[env]}/v1/dataset/{dataset['id']}... \")\n",
    "                status = deleteAssets(f\"{serverUrl[env]}/v1/dataset/{dataset['id']}\", headers)\n",
    "                    \n",
    "            except re.exceptions.HTTPError as err:\n",
    "                logger.error(err)\n",
    "                pass\n",
    "    else:\n",
    "        print('nothing was deleted')\n",
    "\n",
    "def assetIdToBeSync(sync, syncList, assetToSync, fromEnv, toEnv):\n",
    "    '''\n",
    "    controls the asset id to be sync\n",
    "    '''\n",
    "    if sync:\n",
    "        assetId = False\n",
    "        for asset in syncList:\n",
    "            if asset.get('type') == assetToSync.get('type') and asset.get(f'{fromEnv}Id') == assetToSync.get('id'):\n",
    "                assetId = asset.get(f'{toEnv}Id')\n",
    "        return assetId\n",
    "\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def copyAssets(assetList, sync=False, removeAssets=False, fromEnv='prod', toEnv='staging'):\n",
    "    '''\n",
    "    Creates a new copy or syncs the assets that we set up in the fromEnv into the destination Env \n",
    "    '''\n",
    "    if fromEnv == toEnv:\n",
    "        raise NameError(f'fromEnv:{fromEnv} and toEnv:{toEnv} cannot be the same')\n",
    "        \n",
    "    if not assetList or len(assetList) == 0:\n",
    "        raise IndexError(f'asset list is empty or not defined')\n",
    "        \n",
    "    \n",
    "    dataAssets = []  \n",
    "\n",
    "    if sync:\n",
    "        newDatasetList = [asset[f'{fromEnv}Id'] for asset in assetList if asset['type'] == 'dataset']\n",
    "        dataAssets = getAssetList(fromEnv, newDatasetList)\n",
    "    else:   \n",
    "        dataAssets = getAssetList(fromEnv, assetList)\n",
    "\n",
    "    # @TODO:\n",
    "    # Improve loop performance with multiprocessing\n",
    "    # move loops into reusable function based on type\n",
    "    # For sync only patch updated data\n",
    "\n",
    "    for dataset in dataAssets['data']:\n",
    "        try:\n",
    "            print(f'{bcolors.OKBLUE}Preparing to {\"sync\" if sync else \"copy\"} from {fromEnv} to {toEnv}...{bcolors.ENDC}')\n",
    "            \n",
    "            resources = [] # Move this to dataset level as syncfiles are created per dataset now.\n",
    "            \n",
    "            toDatasetId = assetIdToBeSync(sync, assetList, dataset, fromEnv, toEnv)\n",
    "            if toDatasetId:\n",
    "                logger.info(f'sync [{fromEnv}]dataset: {dataset.get(\"id\")}')\n",
    "                logger.info(f'with [{toEnv}]dataset: {toDatasetId}')\n",
    "            newDataset = recreateDataset(dataset, toEnv, toDatasetId)\n",
    "\n",
    "            resources.append({\n",
    "                'type': 'dataset',\n",
    "                f'{fromEnv}Id':dataset.get('id'),\n",
    "                f'{toEnv}Id': newDataset['data'].get('id')\n",
    "            })\n",
    "\n",
    "            for vocabulary in dataset['attributes'].get('vocabulary'):\n",
    "                newVocabulary = recreateVocabulary(newDataset['data'].get('id'), vocabulary, toEnv)\n",
    "                \n",
    "                resources.append({\n",
    "                'type': 'vocabulary',\n",
    "                f'{fromEnv}Id':vocabulary.get('id'),\n",
    "                f'{toEnv}Id': newVocabulary['data']\n",
    "            })\n",
    "\n",
    "            # sync layers\n",
    "            for layer in dataset['attributes'].get('layer'):\n",
    "                \n",
    "                toLayerId = assetIdToBeSync(sync, assetList, layer, fromEnv, toEnv)\n",
    "                if toLayerId:\n",
    "                    logger.info(f'sync [{fromEnv}]layer: {layer.get(\"id\")}')\n",
    "                    logger.info(f'with [{toEnv}]layer: {toLayerId}')\n",
    "                \n",
    "                newLayer = recreateLayer(newDataset['data'].get('id'), layer, toEnv, toLayerId)\n",
    "                \n",
    "                resources.append({\n",
    "                'type': 'layer',\n",
    "                f'{fromEnv}Id':layer.get('id'),\n",
    "                f'{toEnv}Id': newLayer['data'].get('id')\n",
    "                })\n",
    "                \n",
    "                fromLayerMetadata = getSubAssetMetadata(dataset.get(\"id\"), layerId=layer.get(\"id\"), fromEnv=fromEnv)\n",
    "                \n",
    "                if fromLayerMetadata:\n",
    "                    for layerMetadata in fromLayerMetadata.get('data', {}):\n",
    "                        logger.info('creating metadata for layer...')\n",
    "                        newMetadata = recreateMetadata(newDataset['data'].get('id'), layerMetadata, layerId=newLayer['data'].get('id'), toEnv=toEnv)\n",
    "                        \n",
    "                        resources.append({\n",
    "                        'type': 'metadata',\n",
    "                        f'{fromEnv}Id':layerMetadata.get('id'),\n",
    "                        f'{toEnv}Id': newMetadata['data']\n",
    "                        })\n",
    "            # remove toEnv layers that are not on fromEnv using a safe net \n",
    "            if removeAssets:        \n",
    "                for layer in getAssetList(toEnv, [toDatasetId])['data'][0]['attributes'].get('layer'):\n",
    "                    if layer.get(\"id\") not in [asset[f'{toEnv}Id'] for asset in resources if asset['type'] == 'layer']:\n",
    "                        headers = setTokenHeader(toEnv)\n",
    "                        serverUrl = {\n",
    "                            'prod': prod_server,\n",
    "                            'staging': staging_server\n",
    "                        }\n",
    "                        url = f'{serverUrl[toEnv]}/v1/dataset/{toDatasetId}/layer/{layer.get(\"id\")}'\n",
    "                        deleteAssets(url, headers)   \n",
    "            \n",
    "            # sync widgets\n",
    "            for widget in dataset['attributes'].get('widget'):\n",
    "                \n",
    "                toWidgetId = assetIdToBeSync(sync, assetList, widget, fromEnv, toEnv)\n",
    "                if toWidgetId:\n",
    "                    logger.info(f'sync [{fromEnv}]widget: {widget.get(\"id\")}')\n",
    "                    logger.info(f'with [{toEnv}]widget: {toWidgetId}')\n",
    "                    \n",
    "                newWidget = recreateWidget(newDataset['data'].get('id'), widget, toEnv, toWidgetId)                \n",
    "                resources.append({\n",
    "                'type': 'widget',\n",
    "                f'{fromEnv}Id':widget.get('id'),\n",
    "                f'{toEnv}Id': newWidget['data'].get('id')\n",
    "                })\n",
    "                \n",
    "                fromWidgetMetadata = getSubAssetMetadata(dataset.get(\"id\"), widgetId=widget.get(\"id\"), fromEnv=fromEnv)\n",
    "                \n",
    "                if fromWidgetMetadata:\n",
    "                    for widgetMetadata in fromWidgetMetadata.get('data', {}):\n",
    "                        logger.info('creating metadata for widget...')\n",
    "                        newMetadata = recreateMetadata(newDataset['data'].get('id'), widgetMetadata, widgetId=newWidget['data'].get('id'), toEnv=toEnv)\n",
    "                        \n",
    "                        resources.append({\n",
    "                        'type': 'metadata',\n",
    "                        f'{fromEnv}Id':widgetMetadata.get('id'),\n",
    "                        f'{toEnv}Id': newMetadata['data']\n",
    "                        })\n",
    "            # remove toEnv widgets that are not on fromEnv using a safe net\n",
    "            if removeAssets:          \n",
    "                for widget in getAssetList(toEnv, [toDatasetId])['data'][0]['attributes'].get('widget'):\n",
    "                    if widget.get(\"id\") not in [asset[f'{toEnv}Id'] for asset in resources if asset['type'] == 'widget']:\n",
    "                        headers = setTokenHeader(toEnv)\n",
    "                        serverUrl = {\n",
    "                            'prod': prod_server,\n",
    "                            'staging': staging_server\n",
    "                        }\n",
    "                        url = f'{serverUrl[toEnv]}/v1/dataset/{toDatasetId}/widget/{widget.get(\"id\")}'\n",
    "                        deleteAssets(url, headers)       \n",
    "\n",
    "            for metadata in dataset['attributes'].get('metadata'):\n",
    "                logger.info('creating metadata')\n",
    "                newMetadata = recreateMetadata(newDataset['data'].get('id'), metadata, toEnv=toEnv)\n",
    "                \n",
    "                resources.append({\n",
    "                'type': 'metadata',\n",
    "                f'{fromEnv}Id':metadata.get('id'),\n",
    "                f'{toEnv}Id': newMetadata['data']\n",
    "                })\n",
    "            \n",
    "            ## Here we will add the logic to create the sync files.\n",
    "        except NameError or IndexError as e:\n",
    "            logger.error(e)\n",
    "            raise e\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "        # We are assuming that the first item in the resources is a dataset.\n",
    "        filename = f'dataset_sync_files/RW_prod_staging_match_{resources[0][\"prodId\"]}.json'\n",
    "        try:\n",
    "            ### The logic here is try to see if the file already exists and reads it\n",
    "            ### if not it will create it.\n",
    "            fileExists = os.path.exists(filename)\n",
    "            if len(resources) > 0:\n",
    "                with open(filename, 'w+') as outfile:\n",
    "                    if fileExists:\n",
    "                        oldfile = json.load(outfile) # we save here the old sync data.\n",
    "                        # Here there are a couple of drivers: \n",
    "                        # Do we consider that the latest version of sync file generated is the right one? \n",
    "                        # What if there is a failure?\n",
    "                        # Do we want to combine them? on the old code i'm seeing an assumption \n",
    "                        # related metadata being the latest thing.\n",
    "                        difference = list(dictdiffer.diff(resources, oldfile))\n",
    "                        if difference == []:\n",
    "                            break\n",
    "                        else:\n",
    "                            writeOptions = {\n",
    "                                'Y': resources,\n",
    "                                'N': oldfile,\n",
    "                                'M': dictdiffer.patch(difference, resources) \n",
    "                                }\n",
    "                            for diff in difference:         \n",
    "                                print(diff)\n",
    "                            userConfirmation = input(f'{bcolors.WARNING} Do you want to overwrite or merge \\\n",
    "                                {str(oldfile)}  with  {str(resources)}:{bcolors.ENDC} \\\n",
    "                                Y/M/N') or \"N\"\n",
    "                            if userConfirmation not in ('Y', 'N', 'M'):\n",
    "                                raise NameError(f'User confirmation option not valid: {userConfirmation}')\n",
    "                            \n",
    "                            json.dump(writeOptions[userConfirmation], outfile, sort_keys=True)\n",
    "                    else:\n",
    "                        json.dump(resources, outfile, sort_keys=True)\n",
    "                \n",
    "                print(f'{bcolors.OKGREEN}{\"sync\" if sync else \"copy\"} process finished{bcolors.ENDC}')\n",
    "                return filename\n",
    "        except Error as e:\n",
    "            raise e\n",
    "        \n",
    "def syncAssets(syncList, remove = False, fromEnv='prod', toEnv='staging'):\n",
    "    '''\n",
    "    Allows sync of Assets\n",
    "    '''\n",
    "    return copyAssets(syncList, True, remove, fromEnv, toEnv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing\n",
    "## Get list of assets that we want to modify or sync"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List of assets:\n",
    "\n",
    "* `datasetsProd` will contain the id of the assets in productioon that need to be migrated to `staging`. We need to make sure that this list is in sync with the document we have shared with the assets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For testing purposes\n",
    "Dummy assests to create assets in production environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': {'id': '6a3aa408-b3d3-44c6-89b7-93fbfa545489',\n",
       "  'type': 'dataset',\n",
       "  'attributes': {'name': 'This is a test',\n",
       "   'slug': 'This-is-a-test_9',\n",
       "   'type': None,\n",
       "   'subtitle': None,\n",
       "   'application': ['rw'],\n",
       "   'dataPath': None,\n",
       "   'attributesPath': None,\n",
       "   'connectorType': 'rest',\n",
       "   'provider': 'cartodb',\n",
       "   'userId': '57a0aa1071e394dd32ffe137',\n",
       "   'connectorUrl': 'https://wri-rw.carto.com/api/v2/sql?q=select * from air_temo_anomalies',\n",
       "   'sources': [],\n",
       "   'tableName': 'air_temo_anomalies',\n",
       "   'status': 'pending',\n",
       "   'published': False,\n",
       "   'overwrite': False,\n",
       "   'mainDateField': None,\n",
       "   'env': 'production',\n",
       "   'geoInfo': False,\n",
       "   'protected': False,\n",
       "   'legend': {'date': [],\n",
       "    'region': [],\n",
       "    'country': [],\n",
       "    'nested': [],\n",
       "    'integer': [],\n",
       "    'short': [],\n",
       "    'byte': [],\n",
       "    'double': [],\n",
       "    'float': [],\n",
       "    'half_float': [],\n",
       "    'scaled_float': [],\n",
       "    'boolean': [],\n",
       "    'binary': [],\n",
       "    'text': [],\n",
       "    'keyword': []},\n",
       "   'clonedHost': {},\n",
       "   'errorMessage': None,\n",
       "   'taskId': None,\n",
       "   'createdAt': '2021-06-07T09:36:12.332Z',\n",
       "   'updatedAt': '2021-06-07T09:36:12.332Z',\n",
       "   'dataLastUpdated': None,\n",
       "   'widgetRelevantProps': [],\n",
       "   'layerRelevantProps': []}}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dummy data to test the notebook: creation of a dummy dataset with a layer in production.\n",
    "toEnv = 'prod'\n",
    "serverUrl = {\n",
    "        'prod': prod_server,\n",
    "        'staging': staging_server\n",
    "    }\n",
    "headers = setTokenHeader(toEnv)\n",
    "urlDataset = f'{serverUrl[toEnv]}/v1/dataset'\n",
    "bodyDataset = {'dataset':{\n",
    "    'application': ['rw'],\n",
    "    'name': 'This is a test',\n",
    "    'connectorType': 'rest',\n",
    "    'provider': 'cartodb',\n",
    "    'published': False,\n",
    "    'overwrite': False,\n",
    "    'protected':False,\n",
    "    'env': 'production',\n",
    "    'connectorUrl': \"https://wri-rw.carto.com/api/v2/sql?q=select * from air_temo_anomalies\"\n",
    "    }\n",
    "}\n",
    "\n",
    "responseDataset = postAssets(urlDataset, bodyDataset, headers)\n",
    "responseDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': {'id': 'c21dd7ab-e729-4811-9433-8333b1d7c9e9',\n",
       "  'type': 'layer',\n",
       "  'attributes': {'name': 'test-121',\n",
       "   'slug': 'test-121_2',\n",
       "   'dataset': '6a3aa408-b3d3-44c6-89b7-93fbfa545489',\n",
       "   'application': ['rw'],\n",
       "   'iso': [],\n",
       "   'provider': 'cartodb',\n",
       "   'userId': '57a0aa1071e394dd32ffe137',\n",
       "   'default': True,\n",
       "   'protected': False,\n",
       "   'published': False,\n",
       "   'env': 'production',\n",
       "   'layerConfig': {'body': {}},\n",
       "   'legendConfig': {},\n",
       "   'interactionConfig': {},\n",
       "   'applicationConfig': {},\n",
       "   'staticImageConfig': {},\n",
       "   'createdAt': '2021-06-07T09:36:15.327Z',\n",
       "   'updatedAt': '2021-06-07T09:36:15.327Z'}}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urlLayer = f'{urlDataset}/{responseDataset[\"data\"].get(\"id\")}/layer'\n",
    "bodyLayer = {\n",
    "        'application': ['rw'],\n",
    "        'name': 'test-121',\n",
    "        'provider': 'cartodb',\n",
    "        'default': True,\n",
    "        'published': False,\n",
    "        'env': 'production',\n",
    "        'layerConfig': {\n",
    "            \"body\": {}\n",
    "            },\n",
    "        'legendConfig': {},\n",
    "        'interactionConfig': {},\n",
    "        'applicationConfig': {}\n",
    "    }\n",
    "responseLayer = postAssets(urlLayer, bodyLayer, headers)\n",
    "responseLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': {'id': '5f169df0-a293-4588-bbcd-521ee9484cd6',\n",
       "  'type': 'widget',\n",
       "  'attributes': {'name': 'test-121',\n",
       "   'dataset': '6a3aa408-b3d3-44c6-89b7-93fbfa545489',\n",
       "   'slug': 'test-121_2',\n",
       "   'userId': '57a0aa1071e394dd32ffe137',\n",
       "   'application': ['rw'],\n",
       "   'verified': False,\n",
       "   'default': True,\n",
       "   'protected': False,\n",
       "   'defaultEditableWidget': False,\n",
       "   'published': False,\n",
       "   'freeze': False,\n",
       "   'env': 'production',\n",
       "   'widgetConfig': {'body': {}},\n",
       "   'template': False,\n",
       "   'createdAt': '2021-06-07T09:36:17.153Z',\n",
       "   'updatedAt': '2021-06-07T09:36:17.154Z'}}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urlWidget = f'{urlDataset}/{responseDataset[\"data\"].get(\"id\")}/widget'\n",
    "bodyWidget = {\n",
    "        'application': ['rw'],\n",
    "        'name': 'test-121',\n",
    "        'default': True,\n",
    "        'published': False,\n",
    "        'env': 'production',\n",
    "        'widgetConfig': {\n",
    "            \"body\": {}\n",
    "            }\n",
    "    }\n",
    "responseWidget = postAssets(urlWidget, bodyWidget, headers)\n",
    "responseWidget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': [{'id': 'knowledge_graph',\n",
       "   'type': 'vocabulary',\n",
       "   'attributes': {'tags': ['geospatial'],\n",
       "    'name': 'knowledge_graph',\n",
       "    'application': 'rw'}}]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urlVocabulary = f'{urlDataset}/{responseDataset[\"data\"].get(\"id\")}/vocabulary/knowledge_graph'\n",
    "bodyVocabulary = {\n",
    "        'application': 'rw',\n",
    "        'tags':[\"geospatial\"]\n",
    "    }\n",
    "responseVocabulary = postAssets(urlVocabulary, bodyVocabulary, headers)\n",
    "responseVocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': [{'id': '60bde8962852be001ba7e42b',\n",
       "   'type': 'metadata',\n",
       "   'attributes': {'dataset': '6a3aa408-b3d3-44c6-89b7-93fbfa545489',\n",
       "    'application': 'rw',\n",
       "    'resource': {'id': '6a3aa408-b3d3-44c6-89b7-93fbfa545489',\n",
       "     'type': 'dataset'},\n",
       "    'language': 'eng',\n",
       "    'name': 'this is a dummy dataset',\n",
       "    'description': 'Lorem Ipsum',\n",
       "    'createdAt': '2021-06-07T09:36:22.304Z',\n",
       "    'updatedAt': '2021-06-07T09:36:22.304Z',\n",
       "    'status': 'published'}}]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urlMetadataDataset = f'{urlDataset}/{responseDataset[\"data\"].get(\"id\")}/metadata'\n",
    "bodyMetadataDataset = {\n",
    "        'application': 'rw',\n",
    "        'language':'ENG',\n",
    "        'name':'this is a dummy dataset',\n",
    "        'description':'Lorem Ipsum'\n",
    "    }\n",
    "responseMetadataDataset = postAssets(urlMetadataDataset, bodyMetadataDataset, headers)\n",
    "responseMetadataDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urlMetadataLayer = f'{urlDataset}/{responseDataset[\"data\"].get(\"id\")}/layer/{responseLayer[\"data\"].get(\"id\")}/metadata'\n",
    "bodyMetadataLayer = {\n",
    "        'application': 'rw',\n",
    "        'language':'ENG',\n",
    "        'name':'this is a dummy Layer',\n",
    "        'description':'Lorem Ipsum'\n",
    "    }\n",
    "responseMetadataLayer = postAssets(urlMetadataLayer, bodyMetadataLayer, headers)\n",
    "responseMetadataLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': [{'id': '60bde89a3cc064001b3675b9',\n",
       "   'type': 'metadata',\n",
       "   'attributes': {'dataset': '6a3aa408-b3d3-44c6-89b7-93fbfa545489',\n",
       "    'application': 'rw',\n",
       "    'resource': {'id': '5f169df0-a293-4588-bbcd-521ee9484cd6',\n",
       "     'type': 'widget'},\n",
       "    'language': 'eng',\n",
       "    'name': 'this is a dummy widget',\n",
       "    'description': 'Lorem Ipsum',\n",
       "    'createdAt': '2021-06-07T09:36:26.194Z',\n",
       "    'updatedAt': '2021-06-07T09:36:26.194Z',\n",
       "    'status': 'published'}}]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urlMetadatawidget = f'{urlDataset}/{responseDataset[\"data\"].get(\"id\")}/widget/{responseWidget[\"data\"].get(\"id\")}/metadata'\n",
    "bodyMetadatawidget = {\n",
    "        'application': 'rw',\n",
    "        'language':'ENG',\n",
    "        'name':'this is a dummy widget',\n",
    "        'description':'Lorem Ipsum'\n",
    "    }\n",
    "responseMetadatawidget = postAssets(urlMetadatawidget, bodyMetadatawidget, headers)\n",
    "responseMetadatawidget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List of assets:\n",
    "\n",
    "* we need to make sure that this list is in sync with the document we have shared with the assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['6a3aa408-b3d3-44c6-89b7-93fbfa545489']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# in the future we can automate this listing based on the doc using the google sheet api both for writing and reading from\n",
    "# providing a sample of the list by printing it\n",
    "datasetsProd = [responseDataset['data']['id']]\n",
    "datasetsProd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backup Data in both environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#backupAssets('prod')\n",
    "#backupAssets('staging')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only do this if you want to clean data in staging. \n",
    "* You will need to be logged in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#deleteDataFrom()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy resources from production to staging. \n",
    "The running time will depend on the size of the asset.   \n",
    "Running this cell is only needed to create new assets from `production` to `staging`.\n",
    "A json file is created with a unique name in local. The json files contains for each assest:\n",
    "- type: this can be a \"layer\", a \"dataset\", a \"widget\", \"vocabulary\", \"metadata\"\n",
    "- prodId: the id of the item in `production`\n",
    "- stagingId: the id of the item in `staging`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter the API ID of the dataset on production to copy/sync here\n",
    "prod_API_ID = ['']# ex: '79e06dd8-a2ae-45eb-8e99-e73bc87ec946'\n",
    "# keep the syncFile list empty\n",
    "syncFile = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94mPreparing to copy from prod to staging...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:response: \n",
      "ERROR:root:<Response [400]>\n",
      "ERROR:root:url: \n",
      "ERROR:root:https://staging-api.resourcewatch.org/v1/dataset/find-by-ids\n",
      "ERROR:root:body: \n",
      "ERROR:root:{\"ids\": [null]}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating sync file with name: dataset_sync_files/RW_prod_staging_match_42859b52-31f2-419c-ac14-8b0cbd6bbb6f.json\n",
      "\u001b[92mcopy process finished\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:sync [prod]dataset: 42859b52-31f2-419c-ac14-8b0cbd6bbb6f\n",
      "INFO:root:with [staging]dataset: e95fe72e-eb7f-486c-ad0e-b0cc52ac3b94\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94mPreparing to sync from prod to staging...\u001b[0m\n",
      "update sync file dataset_sync_files/RW_prod_staging_match_42859b52-31f2-419c-ac14-8b0cbd6bbb6f.json\n",
      "\u001b[92msync process finished\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:sync [prod]dataset: 42859b52-31f2-419c-ac14-8b0cbd6bbb6f\n",
      "INFO:root:with [staging]dataset: e95fe72e-eb7f-486c-ad0e-b0cc52ac3b94\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94mPreparing to sync from prod to staging...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:sync [prod]widget: 2cb5af4f-2bfc-49f3-9f99-ac415e98c7db\n",
      "INFO:root:with [staging]widget: 1804d8e0-0de5-4b9a-8ecd-b55c9ff176fb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "update sync file dataset_sync_files/RW_prod_staging_match_42859b52-31f2-419c-ac14-8b0cbd6bbb6f.json\n",
      "\u001b[92msync process finished\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# copy a dataset on production to staging\n",
    "for datasetId in prod_API_ID:\n",
    "    syncFile.append(copyAssets([datasetId], False, fromEnv='prod', toEnv = 'staging'))\n",
    "for syncfile in syncFile:\n",
    "    with open(syncfile) as json_file:\n",
    "        syncList = json.load(json_file)\n",
    "    syncAssets(syncList, fromEnv='prod', toEnv='staging')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open sync list of assets, match items with list and update them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:sync [prod]dataset: 42859b52-31f2-419c-ac14-8b0cbd6bbb6f\n",
      "INFO:root:with [staging]dataset: 05f90e71-fef4-445c-82d9-65e77d732494\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94mPreparing to sync from prod to staging...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:sync [prod]widget: 2cb5af4f-2bfc-49f3-9f99-ac415e98c7db\n",
      "INFO:root:with [staging]widget: eedaa69b-7d14-4541-9a0c-1033bcddd072\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "update sync file dataset_sync_files/RW_prod_staging_match_42859b52-31f2-419c-ac14-8b0cbd6bbb6f.json\n",
      "\u001b[92msync process finished\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# sync dataset production <> staging\n",
    "# use the printed json filename in the previous cell\n",
    "if len(syncFile)==0:\n",
    "    syncFile = [f'dataset_sync_files/RW_prod_staging_match_{datasetId}.json' for datasetId in prod_API_ID]\n",
    "for syncfile in syncFile:\n",
    "    with open(syncfile) as json_file:\n",
    "        syncList = json.load(json_file)\n",
    "\n",
    "    syncAssets(syncList, fromEnv='prod', toEnv='staging')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mAre you sure you want to delete         ['6a3aa408-b3d3-44c6-89b7-93fbfa545489'] in prod:\u001b[0m         Y/n Y\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:deleting https://api.resourcewatch.org/v1/dataset/6a3aa408-b3d3-44c6-89b7-93fbfa545489... \n"
     ]
    }
   ],
   "source": [
    "# delete testing datasets from both envs after testing:\n",
    "deleteDataFrom('prod', [responseDataset['data']['id']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:deleting https://staging-api.resourcewatch.org/v1/dataset/e95fe72e-eb7f-486c-ad0e-b0cc52ac3b94... \n"
     ]
    }
   ],
   "source": [
    "deleteDataFrom('staging', [syncList[0]['stagingId']])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1936b053440c27f530542f53326030d97194af8aa0e5ac751988556632f9c990"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
