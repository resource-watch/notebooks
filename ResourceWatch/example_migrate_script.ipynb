{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Migration and sync of assets between prod and staging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently the production API is the one that has the latest updated data by the WRI team. \n",
    "This notebook copies assets from `production` to `staging` maintening the match between IDs. Optionally, it would be possible to copy assets back from `staging` to `production`. \n",
    "\n",
    "### Steps:\n",
    "1. upload/update assest to `production`\n",
    "2. make a copy of the assests from `production` to `staging` using this script\n",
    "3. synchronise the ids of the assets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "1. run the `Functions`.\n",
    "2. create a list with the assets urls to copy.\n",
    "3. `Processing` has the steps to carry out the migration. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions\n",
    "These are the functions we need to create and synchronise assets from `staging` to `production`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import requests as re\n",
    "import json\n",
    "from datetime import datetime\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "staging_server = \"https://staging-api.globalforestwatch.org\"\n",
    "prod_server = \"https://api.resourcewatch.org\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class bcolors:\n",
    "    HEADER = '\\033[95m'\n",
    "    OKBLUE = '\\033[94m'\n",
    "    OKCYAN = '\\033[96m'\n",
    "    OKGREEN = '\\033[92m'\n",
    "    WARNING = '\\033[93m'\n",
    "    FAIL = '\\033[91m'\n",
    "    ENDC = '\\033[0m'\n",
    "    BOLD = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auth(env='prod'):\n",
    "    serverUrl = {\n",
    "        'prod': prod_server,\n",
    "        'staging': staging_server\n",
    "    }\n",
    "    print(f'You are login into {bcolors.HEADER}{bcolors.BOLD}{env}{bcolors.ENDC}')\n",
    "    with re.Session() as s:\n",
    "        headers = {'Content-Type': 'application/json'}\n",
    "        payload = json.dumps({ 'email': f'{input(f\"Email: \")}',\n",
    "                               'password': f'{getpass.getpass(prompt=\"Password: \")}'})\n",
    "        response = s.post(f'{serverUrl[env]}/auth/login',  headers = headers,  data = payload)\n",
    "        response.raise_for_status()\n",
    "        print(f'{bcolors.OKGREEN}Successfully logged into {env}{bcolors.ENDC}')\n",
    "    return response.json().get('data').get('token')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are login into \u001b[95m\u001b[1mstaging\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Email:  greta.carrete@vizzuality.com\n",
      "Password:  ···············\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mSuccessfully logged into staging\u001b[0m\n",
      "You are login into \u001b[95m\u001b[1mprod\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Email:  greta.carrete@vizzuality.com\n",
      "Password:  ···············\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mSuccessfully logged into prod\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "token = {\n",
    "    'staging': auth('staging'),\n",
    "    'prod':auth('prod')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @TODO \n",
    "# * Migrate one day the body payloads to data model classes and refactor to classes following inheritance and recursive property copies\n",
    "# * Type function with Mypy\n",
    "# * Add proper method descriptions\n",
    "# * Refactor methods to reuse more code\n",
    "#from typing import List\n",
    "#from pydantic import BaseModel, parse_obj_as\n",
    "# class DatasetModel(BaseModel):\n",
    "\n",
    "# class LayerModel(BaseModel):\n",
    "\n",
    "# class widgetModel(BaseModel):\n",
    "\n",
    "# class metadataModel(BaseModel):\n",
    "     \n",
    "# class vocabularyModel(BaseModel):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setTokenHeader(env, token=token):\n",
    "    '''\n",
    "    set up the token\n",
    "    '''\n",
    "    return {'Authorization':f'Bearer {token[env]}', \n",
    "            'Content-Type': 'application/json'}\n",
    "\n",
    "def logResponseErrors(status_code, response = None, url = None, body = None):\n",
    "    '''\n",
    "    log errors in http calls\n",
    "    '''\n",
    "    if status_code !=200:\n",
    "        logging.error(response)\n",
    "        logging.error(response.text) if response else None\n",
    "        logging.error(url) if url else None\n",
    "        logging.error(json.dumps(body)) if body else None\n",
    "    \n",
    "\n",
    "def getAssets(url, payload):\n",
    "    '''\n",
    "    Get asset operation\n",
    "    '''\n",
    "    response = re.get(url, payload)\n",
    "    \n",
    "    logResponseErrors(response.status_code, response, url, payload)\n",
    "    \n",
    "    response.raise_for_status()\n",
    "    \n",
    "    return response.json()\n",
    "\n",
    "def deleteAssets(url, headers):\n",
    "    '''\n",
    "    delete asset operation\n",
    "    '''\n",
    "    response = re.delete(url, headers = headers)\n",
    "    \n",
    "    logResponseErrors(response.status_code, response, url)\n",
    "    \n",
    "    response.raise_for_status()\n",
    "    \n",
    "    return response.status_code\n",
    "\n",
    "def postAssets(url, body, headers, payloads = None):\n",
    "    '''\n",
    "    create asset operation\n",
    "    '''    \n",
    "    response = re.post(url, params = payloads, data=json.dumps(body), headers = headers)\n",
    "    \n",
    "    logResponseErrors(response.status_code, response, url, body)\n",
    "    \n",
    "    response.raise_for_status()\n",
    "    \n",
    "    return response.json()\n",
    "\n",
    "def updateAssets(url, body, headers):\n",
    "    '''\n",
    "    patch asset operation\n",
    "    '''\n",
    "    response = re.patch(url, data=json.dumps(body), headers = headers)\n",
    "    \n",
    "    logResponseErrors(response.status_code, response, url, body)\n",
    "    \n",
    "    response.raise_for_status()\n",
    "    \n",
    "    return response.json()\n",
    "\n",
    "def upsert(conditon = False):\n",
    "    '''\n",
    "    Return an update/post operation base on a condition\n",
    "    '''\n",
    "    if conditon:\n",
    "        return updateAssets\n",
    "    else:\n",
    "        return postAssets\n",
    "    \n",
    "def recreateDataset(dataset, toEnv = 'prod', destinationDatasetId = None):\n",
    "    '''\n",
    "    Copy the dataset from one env to the other\n",
    "    '''\n",
    "    \n",
    "    serverUrl = {\n",
    "        'prod': prod_server,\n",
    "        'staging': staging_server\n",
    "    }\n",
    "    if dataset.get('type')!='dataset':\n",
    "        return None\n",
    "    \n",
    "    url = f'{serverUrl[toEnv]}/v1/dataset'\n",
    "    \n",
    "    if destinationDatasetId:\n",
    "        url = f'{url}/{destinationDatasetId}' \n",
    "        \n",
    "    body = {'dataset':{\n",
    "        'application': dataset['attributes'].get('application'),\n",
    "        'name': dataset['attributes'].get('name'),\n",
    "        'connectorType': dataset['attributes'].get('connectorType'),\n",
    "        'provider': dataset['attributes'].get('provider'),\n",
    "        'published': dataset['attributes'].get('published'),\n",
    "        'overwrite': dataset['attributes'].get('overwrite'),\n",
    "        'env': dataset['attributes'].get('env'),\n",
    "        'geoInfo': dataset['attributes'].get('geoInfo'),\n",
    "        'protected': dataset['attributes'].get('protected'),\n",
    "        'legend': dataset['attributes'].get('legend'),\n",
    "        'widgetRelevantProps': dataset['attributes'].get('widgetRelevantProps'),\n",
    "        'layerRelevantProps': dataset['attributes'].get('layerRelevantProps')\n",
    "        }\n",
    "    }\n",
    "    headers = setTokenHeader(toEnv)\n",
    "    \n",
    "    if dataset['attributes'].get('provider') == 'cartodb':\n",
    "            body['dataset']['connectorUrl'] =  dataset['attributes'].get('connectorUrl')\n",
    "    \n",
    "    if dataset['attributes'].get('provider') == 'gee':\n",
    "            body['dataset']['tableName'] =  dataset['attributes'].get('tableName')\n",
    "    if dataset['attributes'].get('mainDateField'):\n",
    "        body['dataset']['mainDateField'] = dataset['attributes'].get('mainDateField')\n",
    "    \n",
    "    \n",
    "    response = upsert(destinationDatasetId)\n",
    "    \n",
    "    logger.debug(response)\n",
    "    if destinationDatasetId:       \n",
    "        return response(url, body['dataset'], headers)\n",
    "    else:\n",
    "        return response(url, body, headers)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "def recreateLayer(datasetId, layer, toEnv = 'prod', destinationLayerId = None):\n",
    "    '''\n",
    "    Copy the layer from one env to the other\n",
    "    '''\n",
    "    \n",
    "    serverUrl = {\n",
    "        'prod': prod_server,\n",
    "        'staging': staging_server\n",
    "    }\n",
    "    if layer.get('type')!='layer':\n",
    "        return None\n",
    "    \n",
    "    headers = setTokenHeader(toEnv)\n",
    "    url = f'{serverUrl[toEnv]}/v1/dataset/{datasetId}/layer'\n",
    "    \n",
    "    if destinationLayerId:\n",
    "        url = f'{url}/{destinationLayerId}'\n",
    "\n",
    "    body = {\n",
    "        'application': layer['attributes'].get('application'),\n",
    "        'name': layer['attributes'].get('name'),\n",
    "        'iso': layer['attributes'].get('iso'),\n",
    "        'provider': layer['attributes'].get('provider'),\n",
    "        'default': layer['attributes'].get('default'),\n",
    "        'protected': layer['attributes'].get('protected'),\n",
    "        'published': layer['attributes'].get('published'),\n",
    "        'env': layer['attributes'].get('env'),\n",
    "        'description': layer['attributes'].get('description'),\n",
    "        'layerConfig': layer['attributes'].get('layerConfig'),\n",
    "        'legendConfig': layer['attributes'].get('legendConfig'),\n",
    "        'interactionConfig': layer['attributes'].get('interactionConfig'),\n",
    "        'applicationConfig': layer['attributes'].get('applicationConfig'),\n",
    "        'staticImageConfig': layer['attributes'].get('staticImageConfig')\n",
    "    }\n",
    "    \n",
    "    response = upsert(destinationLayerId)\n",
    "    return response(url, body, headers)\n",
    "\n",
    "def recreateWidget(datasetId, widget, toEnv = 'prod', destinationWidgetId = None):\n",
    "    '''\n",
    "    Copy the widget from one env to the other\n",
    "    '''\n",
    "    \n",
    "    serverUrl = {\n",
    "        'prod': prod_server,\n",
    "        'staging': staging_server\n",
    "    }\n",
    "    \n",
    "    if widget.get('type')!='widget':\n",
    "        return None\n",
    "    \n",
    "    headers = setTokenHeader(toEnv)\n",
    "    url = f'{serverUrl[toEnv]}/v1/dataset/{datasetId}/widget'\n",
    "    \n",
    "    if destinationWidgetId:\n",
    "        url = f'{url}/{destinationWidgetId}'\n",
    "    \n",
    "    body = {\n",
    "        'application': widget['attributes'].get('application'),\n",
    "        'name': widget['attributes'].get('name'),\n",
    "        'description': widget['attributes'].get('description'),\n",
    "        'verified': widget['attributes'].get('verified'),\n",
    "        'default': widget['attributes'].get('default'),\n",
    "        'protected': widget['attributes'].get('protected'),\n",
    "        'defaultEditableWidget': widget['attributes'].get('defaultEditableWidget'),\n",
    "        'published': widget['attributes'].get('published'),\n",
    "        'freeze': widget['attributes'].get('freeze'),\n",
    "        'env': widget['attributes'].get('env'),\n",
    "        'queryUrl': widget['attributes'].get('queryUrl'),\n",
    "        'widgetConfig': widget['attributes'].get('widgetConfig'),\n",
    "        'template': widget['attributes'].get('template'),\n",
    "        'layerId': widget['attributes'].get('layerId')\n",
    "    }\n",
    "    \n",
    "    response = upsert(destinationWidgetId)\n",
    "    return response(url, body, headers)\n",
    "\n",
    "def recreateMetadata(datasetId, metadata, layerId=None, widgetId=None, toEnv = 'prod'):\n",
    "    '''\n",
    "    Copy the metadata from one env to the other\n",
    "    '''\n",
    "    \n",
    "    serverUrl = {\n",
    "        'prod': prod_server,\n",
    "        'staging': staging_server\n",
    "    }\n",
    "    headers = setTokenHeader(toEnv)\n",
    "    \n",
    "    if metadata.get('type')!='metadata':\n",
    "        return None\n",
    "    if layerId and widgetId:\n",
    "        raise Exception(\"layerId and widgetId not allowed at the same time\")\n",
    "    elif layerId:\n",
    "        url = f'{serverUrl[toEnv]}/v1/dataset/{datasetId}/layer/{layerId}/metadata'\n",
    "    elif widgetId:\n",
    "        url = f'{serverUrl[toEnv]}/v1/dataset/{datasetId}/widget/{widgetId}/metadata'\n",
    "    else:\n",
    "        url = f'{serverUrl[toEnv]}/v1/dataset/{datasetId}/metadata'\n",
    "    \n",
    "    body = {\n",
    "        'application': metadata['attributes'].get('application'),\n",
    "        'language': metadata['attributes'].get('language'),\n",
    "        'description': metadata['attributes'].get('description'),\n",
    "        'source': metadata['attributes'].get('source'),\n",
    "        'info': metadata['attributes'].get('info'),\n",
    "    }\n",
    "    if metadata['attributes'].get('name'):\n",
    "        body['name'] = metadata['attributes'].get('name')\n",
    "    \n",
    "    try:\n",
    "        response = upsert()\n",
    "        return response(url, body, headers)\n",
    "    except Exception as e:\n",
    "        response = upsert(True)\n",
    "        return response(url, body, headers)\n",
    "        pass\n",
    "\n",
    "def recreateVocabulary(datasetId, vocabulary, toEnv = 'prod'):\n",
    "    '''\n",
    "    Copy the vocabulary from one env to the other\n",
    "    '''\n",
    "    \n",
    "    serverUrl = {\n",
    "        'prod': prod_server,\n",
    "        'staging': staging_server\n",
    "    }\n",
    "    \n",
    "    if vocabulary.get('type')!='vocabulary':\n",
    "        return None\n",
    "    \n",
    "    headers = setTokenHeader(toEnv)\n",
    "    \n",
    "    url = f\"{serverUrl[toEnv]}/v1/dataset/{datasetId}/vocabulary/{vocabulary['attributes']['name']}\"\n",
    "    body = {\n",
    "        'application': vocabulary['attributes'].get('application'),\n",
    "        'tags': vocabulary['attributes'].get('tags')\n",
    "    }\n",
    "    \n",
    "    response = postAssets(url, body, headers)\n",
    "    return response\n",
    "\n",
    "def getAssetList(fromEnv = 'prod', datasetList=None):\n",
    "    '''\n",
    "    Gets a list of assets from the selected env or from the constrained dataset list\n",
    "    '''\n",
    "    serverUrl = {\n",
    "        'prod': prod_server,\n",
    "        'staging': staging_server\n",
    "    }\n",
    "    headers = setTokenHeader(fromEnv)\n",
    "    url = f'{serverUrl[fromEnv]}/v1/dataset'\n",
    "    payload={\n",
    "        'application':'rw',\n",
    "        'status':'saved',\n",
    "        'published':'false',\n",
    "        'includes':'widget,layer,vocabulary,metadata',\n",
    "        'page[size]':1613982331640\n",
    "    }\n",
    "    if datasetList:\n",
    "        url = f'{serverUrl[fromEnv]}/v1/dataset/find-by-ids'\n",
    "        body = {\n",
    "            'ids': datasetList\n",
    "        }\n",
    "        return postAssets(url, body, headers, payload)\n",
    "    else:\n",
    "        return getAssets(url, payload)\n",
    "    \n",
    "def backupAssets(env = 'prod', datasetList = None):\n",
    "    '''\n",
    "    save a backup of production data just in case we need to recreate it again\n",
    "    '''\n",
    "    data = getAssetList(env, datasetList)\n",
    "    \n",
    "\n",
    "    with open(f'RW_{Env}_backup_{datetime.now().strftime(\"%Y%m%d-%H%M%S\")}.json', 'w') as outfile:\n",
    "        json.dump(data, outfile)\n",
    "\n",
    "def deleteDataFrom(env='staging', datasetList = None):\n",
    "    '''\n",
    "    Deletes all assets from an env.\n",
    "    '''\n",
    "    serverUrl = {\n",
    "        'prod': prod_server,\n",
    "        'staging': staging_server\n",
    "    }\n",
    "    userConfirmation = input(f'{bcolors.WARNING}Are you sure you want to delete \\\n",
    "        {str(datasetList)  if datasetList else \"everything\" } in {env}:{bcolors.ENDC} \\\n",
    "        Y/n') or \"N\"\n",
    "    if userConfirmation == 'Y':\n",
    "        headers = setTokenHeader(env)\n",
    "        data = getAssetList(env, datasetList)\n",
    "        \n",
    "        for dataset in data['data']:\n",
    "            #@TODO: this needs to be reworked a bit\n",
    "            try:\n",
    "                logger.info(f\"deleting {serverUrl[env]}/v1/dataset/{dataset['id']}... \")\n",
    "                status = deleteAssets(f\"{serverUrl[env]}/v1/dataset/{dataset['id']}\", headers)\n",
    "                    \n",
    "            except re.exceptions.HTTPError as err:\n",
    "                logger.error(err)\n",
    "                pass\n",
    "    else:\n",
    "        print('nothing was deleted')\n",
    "\n",
    "def assetIdToBeSync(sync, syncList, assetToSync, fromEnv, toEnv):\n",
    "    '''\n",
    "    controls the asset id to be sync\n",
    "    '''\n",
    "    if sync:\n",
    "        return next((asset.get(f'{toEnv}Id') for asset in syncList \\\n",
    "                     if (asset.get('type') == assetToSync.get('type') \\\n",
    "                         and asset.get(f'{fromEnv}Id') == assetToSync.get('id'))), \n",
    "                    False)\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def copyAssets(assetList, sync=False, fromEnv='prod', toEnv='staging'):\n",
    "    '''\n",
    "    Creates a new copy or syncs the assets that we set up in the fromEnv into the destination Env \n",
    "    '''\n",
    "    if fromEnv == toEnv:\n",
    "        raise NameError(f'fromEnv:{fromEnv} and toEnv:{toEnv} cannot be the same')\n",
    "        \n",
    "    if not assetList or len(assetList) == 0:\n",
    "        raise IndexError(f'asset list is empty or not defined')\n",
    "        \n",
    "    \n",
    "    dataAssets = []    \n",
    "    \n",
    "    if sync:\n",
    "        newDatasetList = [asset[f'{fromEnv}Id'] for asset in assetList if asset['type'] == 'dataset']\n",
    "        dataAssets = getAssetList(fromEnv, newDatasetList)\n",
    "\n",
    "    else:   \n",
    "        dataAssets = getAssetList(fromEnv, assetList)\n",
    "    \n",
    "    try:\n",
    "        logger.info(f'{bcolors.OKBLUE}Preparing to {\"sync\" if sync else \"copy\"} from {fromEnv} to {toEnv}...{bcolors.ENDC}')\n",
    "        resources = []\n",
    "        \n",
    "        # @TODO:\n",
    "        # Improve loop performance with multiprocessing\n",
    "        # move loops into reusable function based on type\n",
    "        # For sync only path updated data\n",
    "        \n",
    "        for dataset in dataAssets['data']:\n",
    "            \n",
    "            toDatasetId = assetIdToBeSync(sync, assetList, dataset, fromEnv, toEnv)\n",
    "            \n",
    "            newDataset = recreateDataset(dataset, toEnv, toDatasetId)\n",
    "\n",
    "            resources.append({\n",
    "                'type': 'dataset',\n",
    "                f'{fromEnv}Id':dataset.get('id'),\n",
    "                f'{toEnv}Id': newDataset['data'].get('id')\n",
    "            })\n",
    "\n",
    "            for vocabulary in dataset['attributes'].get('vocabulary'):\n",
    "                newVocabulary = recreateVocabulary(newDataset['data'].get('id'), vocabulary, toEnv)\n",
    "                \n",
    "                resources.append({\n",
    "                'type': 'vocabulary',\n",
    "                f'{fromEnv}Id':vocabulary.get('id'),\n",
    "                f'{toEnv}Id': newVocabulary['data']\n",
    "            })\n",
    "\n",
    "            for layer in dataset['attributes'].get('layer'):\n",
    "                \n",
    "                toLayerId = assetIdToBeSync(sync, assetList, layer, fromEnv, toEnv)\n",
    "                \n",
    "                newLayer = recreateLayer(newDataset['data'].get('id'), layer, toEnv, toLayerId)\n",
    "                \n",
    "                resources.append({\n",
    "                'type': 'layer',\n",
    "                f'{fromEnv}Id':layer.get('id'),\n",
    "                f'{toEnv}Id': newLayer['data'].get('id')\n",
    "            })\n",
    "\n",
    "            for widget in dataset['attributes'].get('widget'):\n",
    "                \n",
    "                toWidgetId = assetIdToBeSync(sync, assetList, widget, fromEnv, toEnv)\n",
    "                \n",
    "                newWidget = recreateWidget(newDataset['data'].get('id'), widget, toEnv, toWidgetId)\n",
    "                \n",
    "                resources.append({\n",
    "                'type': 'widget',\n",
    "                f'{fromEnv}Id':widget.get('id'),\n",
    "                f'{toEnv}Id': newWidget['data'].get('id')\n",
    "            })\n",
    "\n",
    "            for metadata in dataset['attributes'].get('metadata'):\n",
    "                \n",
    "                newMetadata = recreateMetadata(newDataset['data'].get('id'), metadata, toEnv=toEnv)\n",
    "                \n",
    "                resources.append({\n",
    "                'type': 'metadata',\n",
    "                f'{fromEnv}Id':metadata.get('id'),\n",
    "                f'{toEnv}Id': newMetadata['data']\n",
    "            })\n",
    "    except NameError or IndexError as e:\n",
    "        logger.error(e)\n",
    "        raise e\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    if not sync:\n",
    "        filename = f'RW_prod_staging_match_{datetime.now().strftime(\"%Y%m%d-%H%M%S\")}.json'\n",
    "        logger.info(f'creating sync file with name: {filename}')\n",
    "        with open(filename, 'w') as outfile:\n",
    "            json.dump(resources, outfile)\n",
    "    \n",
    "    logger.info(f'{bcolors.OKGREEN}{\"sync\" if sync else \"copy\"} process finished{bcolors.ENDC}')\n",
    "        \n",
    "def syncAssets(syncList, fromEnv='prod', toEnv='staging'):\n",
    "    '''\n",
    "    Allows sync of Assets\n",
    "    '''\n",
    "    \n",
    "    return copyAssets(syncList, True, fromEnv, toEnv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing\n",
    "## Get list of assets that we want to modify or sync"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List of assets:\n",
    "\n",
    "* `datasetsProd` will contain the id of the assets in productioon that need to be migrated to `staging`. We need to make sure that this list is in sync with the document we have shared with the assets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For testing purposes\n",
    "Dummy assests to create `datasetsProd`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': {'attributes': {'application': ['rw'],\n",
      "                         'attributesPath': None,\n",
      "                         'clonedHost': {},\n",
      "                         'connectorType': 'rest',\n",
      "                         'connectorUrl': 'https://wri-rw.carto.com/api/v2/sql?q=select '\n",
      "                                         '* from air_temo_anomalies',\n",
      "                         'createdAt': '2021-05-20T16:09:57.922Z',\n",
      "                         'dataLastUpdated': None,\n",
      "                         'dataPath': None,\n",
      "                         'env': 'production',\n",
      "                         'errorMessage': None,\n",
      "                         'geoInfo': False,\n",
      "                         'layerRelevantProps': [],\n",
      "                         'legend': {'binary': [],\n",
      "                                    'boolean': [],\n",
      "                                    'byte': [],\n",
      "                                    'country': [],\n",
      "                                    'date': [],\n",
      "                                    'double': [],\n",
      "                                    'float': [],\n",
      "                                    'half_float': [],\n",
      "                                    'integer': [],\n",
      "                                    'keyword': [],\n",
      "                                    'nested': [],\n",
      "                                    'region': [],\n",
      "                                    'scaled_float': [],\n",
      "                                    'short': [],\n",
      "                                    'text': []},\n",
      "                         'mainDateField': None,\n",
      "                         'name': 'This is a test',\n",
      "                         'overwrite': False,\n",
      "                         'protected': False,\n",
      "                         'provider': 'cartodb',\n",
      "                         'published': False,\n",
      "                         'slug': 'This-is-a-test_5',\n",
      "                         'sources': [],\n",
      "                         'status': 'pending',\n",
      "                         'subtitle': None,\n",
      "                         'tableName': 'air_temo_anomalies',\n",
      "                         'taskId': None,\n",
      "                         'type': None,\n",
      "                         'updatedAt': '2021-05-20T16:09:57.922Z',\n",
      "                         'userId': '59db4eace9c1380001d6e4c3',\n",
      "                         'widgetRelevantProps': []},\n",
      "          'id': 'e4409726-1a7a-4267-9385-e4f8c80ced71',\n",
      "          'type': 'dataset'}}\n"
     ]
    }
   ],
   "source": [
    "# Dummy data to test the notebook: creation of a dummy dataset with a layer in production.\n",
    "toEnv = 'prod'\n",
    "serverUrl = {\n",
    "        'prod': prod_server,\n",
    "        'staging': staging_server\n",
    "    }\n",
    "headers = setTokenHeader(toEnv)\n",
    "urlDataset = f'{serverUrl[toEnv]}/v1/dataset'\n",
    "bodyDataset = {'dataset':{\n",
    "    'application': ['rw'],\n",
    "    'name': 'This is a test',\n",
    "    'connectorType': 'rest',\n",
    "    'provider': 'cartodb',\n",
    "    'published': False,\n",
    "    'overwrite': False,\n",
    "    'protected':False,\n",
    "    'env': 'production',\n",
    "    'connectorUrl': \"https://wri-rw.carto.com/api/v2/sql?q=select * from air_temo_anomalies\"\n",
    "    }\n",
    "}\n",
    "\n",
    "responseDataset = postAssets(urlDataset, bodyDataset, headers)\n",
    "responseDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': {'attributes': {'application': ['rw'],\n",
      "                         'applicationConfig': {},\n",
      "                         'createdAt': '2021-05-20T16:10:03.362Z',\n",
      "                         'dataset': 'e4409726-1a7a-4267-9385-e4f8c80ced71',\n",
      "                         'default': True,\n",
      "                         'env': 'production',\n",
      "                         'interactionConfig': {},\n",
      "                         'iso': [],\n",
      "                         'layerConfig': {'body': {}},\n",
      "                         'legendConfig': {},\n",
      "                         'name': 'test-121',\n",
      "                         'protected': False,\n",
      "                         'provider': 'cartodb',\n",
      "                         'published': False,\n",
      "                         'slug': 'test-121',\n",
      "                         'staticImageConfig': {},\n",
      "                         'updatedAt': '2021-05-20T16:10:03.362Z',\n",
      "                         'userId': '59db4eace9c1380001d6e4c3'},\n",
      "          'id': '308cf5f3-781b-4ce7-9174-c1eb1bd63d3e',\n",
      "          'type': 'layer'}}\n"
     ]
    }
   ],
   "source": [
    "urlLayer = f'{urlDataset}/{responseDataset[\"data\"].get(\"id\")}/layer'\n",
    "bodyLayer = {\n",
    "        'application': ['rw'],\n",
    "        'name': 'test-121',\n",
    "        'provider': 'cartodb',\n",
    "        'default': True,\n",
    "        'published': False,\n",
    "        'env': 'production',\n",
    "        'layerConfig': {\n",
    "            \"body\": {}\n",
    "            },\n",
    "        'legendConfig': {},\n",
    "        'interactionConfig': {},\n",
    "        'applicationConfig': {}\n",
    "    }\n",
    "responseLayer = postAssets(urlLayer, bodyLayer, headers)\n",
    "responseLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in the future we can automate this listing based on the doc using the google sheet api both for writing and reading from\n",
    "# providing a sample of the list by printing it\n",
    "datasetsProd = [responseDataset['data']['id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['e4409726-1a7a-4267-9385-e4f8c80ced71']"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasetsProd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backup Data in both environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#backupAssets('prod')\n",
    "#backupAssets('staging')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only do this if you want to clean data in staging. \n",
    "* You will need to be logged in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#deleteDataFrom()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy resources from production to staging. \n",
    "The running time will depend on the size of the asset.   \n",
    "Running this cell is only needed to create new assets from `production` to `staging`.\n",
    "A json file is created with a unique name in local. The json files contains for each assest:\n",
    "- type: this can be a \"layer\", a \"dataset\", a \"widget\", \"vocabulary\", \"metadata\"\n",
    "- prodId: the id of the item in `production`\n",
    "- stagingId: the id of the item in `staging`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:\u001b[94mPreparing to copy from prod to staging...\u001b[0m\n",
      "INFO:root:creating sync file with name: RW_prod_staging_match_20210520-172552.json\n",
      "INFO:root:\u001b[92mcopy process finished\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "copyAssets(datasetsProd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open sync list of assets, match items with list and update them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the printed json filename in the previous cell\n",
    "with open('RW_prod_staging_match_20210520-172552.json') as json_file:\n",
    "    syncList = json.load(json_file)\n",
    "\n",
    "syncAssets(syncList, fromEnv='prod', toEnv='staging')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\u001b[93mAre you sure you want to delete ['e4409726-1a7a-4267-9385-e4f8c80ced71'] in prod:\u001b[0m Y/n Y\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:deleting https://api.resourcewatch.org/v1/dataset/e4409726-1a7a-4267-9385-e4f8c80ced71... \n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\u001b[93mAre you sure you want to delete ['196588b3-7663-4f9c-8209-d037db8bd1bd'] in staging:\u001b[0m Y/n Y\n"
     ]
    }
   ],
   "source": [
    "# delete testing datasets from both envs after testing:\n",
    "# deleteDataFrom('prod', [responseDataset['data']['id']])\n",
    "# deleteDataFrom('staging', [syncList[0]['stagingId']])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}