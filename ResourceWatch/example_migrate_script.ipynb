{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Migration and sync of assets between prod and staging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently the production API is the one that has the latest updated data by the WRI team. \n",
    "This notebook copies assets from `production` to `staging` maintening the match between IDs. Optionally, it would be possible to copy assets back from `staging` to `production`. \n",
    "\n",
    "### Steps:\n",
    "1. upload/update assest to `production`\n",
    "2. make a copy of the assests from `production` to `staging` using this script\n",
    "3. synchronise the ids of the assets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "1. run the `Functions`.\n",
    "2. create a list with the assets urls to copy.\n",
    "3. `Processing` has the steps to carry out the migration. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions\n",
    "These are the functions we need to create and synchronise assets from `staging` to `production`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import requests as re\n",
    "import json\n",
    "from datetime import datetime\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "staging_server = \"https://staging-api.resourcewatch.org\"\n",
    "prod_server = \"https://api.resourcewatch.org\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class bcolors:\n",
    "    HEADER = '\\033[95m'\n",
    "    OKBLUE = '\\033[94m'\n",
    "    OKCYAN = '\\033[96m'\n",
    "    OKGREEN = '\\033[92m'\n",
    "    WARNING = '\\033[93m'\n",
    "    FAIL = '\\033[91m'\n",
    "    ENDC = '\\033[0m'\n",
    "    BOLD = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auth(env='prod'):\n",
    "    serverUrl = {\n",
    "        'prod': prod_server,\n",
    "        'staging': staging_server\n",
    "    }\n",
    "    print(f'You are login into {bcolors.HEADER}{bcolors.BOLD}{env}{bcolors.ENDC}')\n",
    "    with re.Session() as s:\n",
    "        headers = {'Content-Type': 'application/json'}\n",
    "        payload = json.dumps({ 'email': f'{input(f\"Email: \")}',\n",
    "                               'password': f'{getpass.getpass(prompt=\"Password: \")}'})\n",
    "        response = s.post(f'{serverUrl[env]}/auth/login',  headers = headers,  data = payload)\n",
    "        response.raise_for_status()\n",
    "        print(f'{bcolors.OKGREEN}Successfully logged into {env}{bcolors.ENDC}')\n",
    "    return response.json().get('data').get('token')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "You are login into \u001b[95m\u001b[1mstaging\u001b[0m\n",
      "\u001b[92mSuccessfully logged into staging\u001b[0m\n",
      "You are login into \u001b[95m\u001b[1mprod\u001b[0m\n",
      "\u001b[92mSuccessfully logged into prod\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "token = {\n",
    "    'staging': auth('staging'),\n",
    "    'prod':auth('prod')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @TODO \n",
    "# * Migrate one day the body payloads to data model classes and refactor to classes following inheritance and recursive property copies\n",
    "# * Type function with Mypy\n",
    "# * Add proper method descriptions\n",
    "# * Refactor methods to reuse more code\n",
    "#from typing import List\n",
    "#from pydantic import BaseModel, parse_obj_as\n",
    "# class DatasetModel(BaseModel):\n",
    "\n",
    "# class LayerModel(BaseModel):\n",
    "\n",
    "# class widgetModel(BaseModel):\n",
    "\n",
    "# class metadataModel(BaseModel):\n",
    "     \n",
    "# class vocabularyModel(BaseModel):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setTokenHeader(env, token=token):\n",
    "    '''\n",
    "    set up the token\n",
    "    '''\n",
    "    return {'Authorization':f'Bearer {token[env]}', \n",
    "            'Content-Type': 'application/json'}\n",
    "\n",
    "def logResponseErrors(status_code, response = None, url = None, body = None):\n",
    "    '''\n",
    "    log errors in http calls\n",
    "    '''\n",
    "    if status_code !=200:\n",
    "        logging.error('response: ')\n",
    "        logging.error(response)\n",
    "        logging.error(response.text) if response else None\n",
    "        logging.error(response.json()) if response else None\n",
    "        logging.error('url: ')\n",
    "        logging.error(url) if url else None\n",
    "        logging.error('body: ')\n",
    "        logging.error(json.dumps(body)) if body else None\n",
    "    \n",
    "\n",
    "def getAssets(url, payload=None):\n",
    "    '''\n",
    "    Get asset operation\n",
    "    '''\n",
    "    response = re.get(url, payload)\n",
    "    \n",
    "    logResponseErrors(response.status_code, response, url, payload)\n",
    "    \n",
    "    response.raise_for_status()\n",
    "    \n",
    "    return response.json()\n",
    "\n",
    "def deleteAssets(url, headers):\n",
    "    '''\n",
    "    delete asset operation\n",
    "    '''\n",
    "    response = re.delete(url, headers = headers)\n",
    "    \n",
    "    logResponseErrors(response.status_code, response, url)\n",
    "    \n",
    "    response.raise_for_status()\n",
    "    \n",
    "    return response.status_code\n",
    "\n",
    "def postAssets(url, body, headers, payloads = None):\n",
    "    '''\n",
    "    create asset operation\n",
    "    '''    \n",
    "    response = re.post(url, params = payloads, data=json.dumps(body), headers = headers)\n",
    "    \n",
    "    logResponseErrors(response.status_code, response, url, body)\n",
    "    \n",
    "    response.raise_for_status()\n",
    "    \n",
    "    return response.json()\n",
    "\n",
    "def updateAssets(url, body, headers):\n",
    "    '''\n",
    "    patch asset operation\n",
    "    '''\n",
    "    response = re.patch(url, data=json.dumps(body), headers = headers)\n",
    "    \n",
    "    logResponseErrors(response.status_code, response, url, body)\n",
    "    \n",
    "    response.raise_for_status()\n",
    "    \n",
    "    return response.json()\n",
    "\n",
    "def copyAssetBody(asset, excludeList=['createdAt', 'updatedAt','clonedHost', 'errorMessage', 'taskId', 'status', 'sources',\n",
    "                                      'userId', 'slug', 'dataset', 'layer', 'widget', 'metadata', 'vocabulary']):\n",
    "    '''\n",
    "    Copy a body dict to a new dict excluding some keys or not defined values.\n",
    "    '''\n",
    "    response = {}\n",
    "    response.update(asset)\n",
    "    \n",
    "    for key, value in asset.items():\n",
    "        if (key in excludeList or value is None or (type(value) == dict and len(value) == 0) ):\n",
    "            response.pop(key, None)\n",
    "    \n",
    "    if 'provider' in response.keys() and response['provider'] =='cartodb':\n",
    "        response.pop('tableName', None)\n",
    "    \n",
    "    return response\n",
    "\n",
    "def upsert(conditon = False):\n",
    "    '''\n",
    "    Return an update/post operation base on a condition\n",
    "    '''\n",
    "    if conditon:\n",
    "        return updateAssets\n",
    "    else:\n",
    "        return postAssets\n",
    "    \n",
    "def recreateDataset(dataset, toEnv = 'staging', destinationDatasetId = None):\n",
    "    '''\n",
    "    Copy the dataset from one env to the other\n",
    "    '''\n",
    "    \n",
    "    serverUrl = {\n",
    "        'prod': prod_server,\n",
    "        'staging': staging_server\n",
    "    }\n",
    "    if dataset.get('type')!='dataset':\n",
    "        return None\n",
    "    \n",
    "    url = f'{serverUrl[toEnv]}/v1/dataset'\n",
    "    \n",
    "    if destinationDatasetId:\n",
    "        url = f'{url}/{destinationDatasetId}' \n",
    "        \n",
    "    body = {'dataset': copyAssetBody(dataset.get('attributes'))}\n",
    "    \n",
    "    logger.debug(body)\n",
    "    \n",
    "    headers = setTokenHeader(toEnv)\n",
    "    \n",
    "    response = upsert(destinationDatasetId)\n",
    "    \n",
    "    logger.debug(response)\n",
    "    if destinationDatasetId:       \n",
    "        return response(url, body['dataset'], headers)\n",
    "    else:\n",
    "        return response(url, body, headers)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "def recreateLayer(datasetId, layer, toEnv = 'staging', destinationLayerId = None):\n",
    "    '''\n",
    "    Copy the layer from one env to the other\n",
    "    '''\n",
    "    \n",
    "    serverUrl = {\n",
    "        'prod': prod_server,\n",
    "        'staging': staging_server\n",
    "    }\n",
    "    if layer.get('type')!='layer':\n",
    "        return None\n",
    "    \n",
    "    headers = setTokenHeader(toEnv)\n",
    "    url = f'{serverUrl[toEnv]}/v1/dataset/{datasetId}/layer'\n",
    "    \n",
    "    if destinationLayerId:\n",
    "        url = f'{url}/{destinationLayerId}'\n",
    "\n",
    "    body = copyAssetBody(layer.get('attributes'))\n",
    "    \n",
    "    response = upsert(destinationLayerId)\n",
    "    \n",
    "    return response(url, body, headers)\n",
    "\n",
    "def recreateWidget(datasetId, widget, toEnv = 'staging', destinationWidgetId = None):\n",
    "    '''\n",
    "    Copy the widget from one env to the other\n",
    "    '''\n",
    "    \n",
    "    serverUrl = {\n",
    "        'prod': prod_server,\n",
    "        'staging': staging_server\n",
    "    }\n",
    "    \n",
    "    if widget.get('type')!='widget':\n",
    "        return None\n",
    "    \n",
    "    headers = setTokenHeader(toEnv)\n",
    "    url = f'{serverUrl[toEnv]}/v1/dataset/{datasetId}/widget'\n",
    "    \n",
    "    if destinationWidgetId:\n",
    "        url = f'{url}/{destinationWidgetId}'\n",
    "    \n",
    "    body = copyAssetBody(widget.get('attributes'))\n",
    "    \n",
    "    response = upsert(destinationWidgetId)\n",
    "    \n",
    "    return response(url, body, headers)\n",
    "\n",
    "def getSubAssetMetadata(datasetId, layerId=None, widgetId=None, fromEnv = 'prod'):\n",
    "    '''\n",
    "    Get metadata that is not given back from main call\n",
    "    '''\n",
    "    serverUrl = {\n",
    "        'prod': prod_server,\n",
    "        'staging': staging_server\n",
    "    }\n",
    "    \n",
    "    if (layerId and widgetId) or (not layerId and not widgetId):\n",
    "        raise Exception(\"layerId and widgetId not allowed at the same time\")\n",
    "    elif layerId:\n",
    "        url = f'{serverUrl[fromEnv]}/v1/dataset/{datasetId}/layer/{layerId}/metadata'\n",
    "    elif widgetId:\n",
    "        url = f'{serverUrl[fromEnv]}/v1/dataset/{datasetId}/widget/{widgetId}/metadata'\n",
    "        \n",
    "    try:\n",
    "        return getAssets(url)\n",
    "    except Exception as e:\n",
    "        logger.info('Get operation was not successfull')\n",
    "        logger.error(f'{e}')\n",
    "        return None\n",
    "        pass\n",
    "    \n",
    "\n",
    "def recreateMetadata(datasetId, metadata, layerId=None, widgetId=None, toEnv = 'prod'):\n",
    "    '''\n",
    "    Copy the metadata from one env to the other\n",
    "    '''\n",
    "    \n",
    "    serverUrl = {\n",
    "        'prod': prod_server,\n",
    "        'staging': staging_server\n",
    "    }\n",
    "    headers = setTokenHeader(toEnv)\n",
    "    \n",
    "    if metadata.get('type')!='metadata':\n",
    "        return None\n",
    "    if layerId and widgetId:\n",
    "        raise Exception(\"layerId and widgetId not allowed at the same time\")\n",
    "    elif layerId:\n",
    "        url = f'{serverUrl[toEnv]}/v1/dataset/{datasetId}/layer/{layerId}/metadata'\n",
    "    elif widgetId:\n",
    "        url = f'{serverUrl[toEnv]}/v1/dataset/{datasetId}/widget/{widgetId}/metadata'\n",
    "    else:\n",
    "        url = f'{serverUrl[toEnv]}/v1/dataset/{datasetId}/metadata'\n",
    "    \n",
    "    body = copyAssetBody(metadata.get('attributes'))\n",
    "    \n",
    "    try:\n",
    "        response = upsert()\n",
    "        return response(url, body, headers)\n",
    "    except Exception as e:\n",
    "        print('Post operation was not succesfull, trying to update instead')\n",
    "        response = upsert(True)\n",
    "        return response(url, body, headers)\n",
    "        pass\n",
    "\n",
    "def recreateVocabulary(datasetId, vocabulary, toEnv = 'prod'):\n",
    "    '''\n",
    "    Copy the vocabulary from one env to the other\n",
    "    '''\n",
    "    \n",
    "    serverUrl = {\n",
    "        'prod': prod_server,\n",
    "        'staging': staging_server\n",
    "    }\n",
    "    \n",
    "    if vocabulary.get('type')!='vocabulary':\n",
    "        return None\n",
    "    \n",
    "    headers = setTokenHeader(toEnv)\n",
    "    \n",
    "    url = f\"{serverUrl[toEnv]}/v1/dataset/{datasetId}/vocabulary/{vocabulary['attributes']['name']}\"\n",
    "    body = {\n",
    "        'application': vocabulary['attributes'].get('application'),\n",
    "        'tags': vocabulary['attributes'].get('tags')\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = upsert()\n",
    "        return response(url, body, headers)\n",
    "    except Exception as e:\n",
    "        print(f'{bcolors.WARNING}Post operation was not succesfull, trying to update instead{bcolors.ENDC}')\n",
    "        response = upsert(True)\n",
    "        return response(url, body, headers)\n",
    "        pass\n",
    "\n",
    "def getAssetList(fromEnv = 'prod', datasetList=None):\n",
    "    '''\n",
    "    Gets a list of assets from the selected env or from the constrained dataset list\n",
    "    '''\n",
    "    serverUrl = {\n",
    "        'prod': prod_server,\n",
    "        'staging': staging_server\n",
    "    }\n",
    "    headers = setTokenHeader(fromEnv)\n",
    "    url = f'{serverUrl[fromEnv]}/v1/dataset'\n",
    "    payload={\n",
    "        'application':'rw',\n",
    "        'status':'saved',\n",
    "        'includes':'widget,layer,vocabulary,metadata',\n",
    "        'page[size]':1613982331640\n",
    "    }\n",
    "    if datasetList:\n",
    "        url = f'{serverUrl[fromEnv]}/v1/dataset/find-by-ids'\n",
    "        body = {\n",
    "            'ids': datasetList\n",
    "        }\n",
    "        return postAssets(url, body, headers, payload)\n",
    "    else:\n",
    "        return getAssets(url, payload)\n",
    "    \n",
    "def backupAssets(env = 'prod', datasetList = None):\n",
    "    '''\n",
    "    save a backup of production data just in case we need to recreate it again\n",
    "    '''\n",
    "    data = getAssetList(env, datasetList)\n",
    "    \n",
    "\n",
    "    with open(f'RW_{Env}_backup_{datetime.now().strftime(\"%Y%m%d-%H%M%S\")}.json', 'w') as outfile:\n",
    "        json.dump(data, outfile)\n",
    "\n",
    "def deleteDataFrom(env='staging', datasetList = None):\n",
    "    '''\n",
    "    Deletes all assets from an env.\n",
    "    '''\n",
    "    serverUrl = {\n",
    "        'prod': prod_server,\n",
    "        'staging': staging_server\n",
    "    }\n",
    "    userConfirmation = input(f'{bcolors.WARNING}Are you sure you want to delete \\\n",
    "        {str(datasetList)  if datasetList else \"everything\" } in {env}:{bcolors.ENDC} \\\n",
    "        Y/n') or \"N\"\n",
    "    if userConfirmation == 'Y':\n",
    "        headers = setTokenHeader(env)\n",
    "        data = getAssetList(env, datasetList)\n",
    "        \n",
    "        for dataset in data['data']:\n",
    "            #@TODO: this needs to be reworked a bit\n",
    "            try:\n",
    "                logger.info(f\"deleting {serverUrl[env]}/v1/dataset/{dataset['id']}... \")\n",
    "                status = deleteAssets(f\"{serverUrl[env]}/v1/dataset/{dataset['id']}\", headers)\n",
    "                    \n",
    "            except re.exceptions.HTTPError as err:\n",
    "                logger.error(err)\n",
    "                pass\n",
    "    else:\n",
    "        print('nothing was deleted')\n",
    "\n",
    "def assetIdToBeSync(sync, syncList, assetToSync, fromEnv, toEnv):\n",
    "    '''\n",
    "    controls the asset id to be sync\n",
    "    '''\n",
    "    if sync:\n",
    "        assetId = False\n",
    "        for asset in syncList:\n",
    "            if asset.get('type') == assetToSync.get('type') and asset.get(f'{fromEnv}Id') == assetToSync.get('id'):\n",
    "                assetId = asset.get(f'{toEnv}Id')\n",
    "        return assetId\n",
    "\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def copyAssets(assetList, sync=False, fromEnv='prod', toEnv='staging'):\n",
    "    '''\n",
    "    Creates a new copy or syncs the assets that we set up in the fromEnv into the destination Env \n",
    "    '''\n",
    "    if fromEnv == toEnv:\n",
    "        raise NameError(f'fromEnv:{fromEnv} and toEnv:{toEnv} cannot be the same')\n",
    "        \n",
    "    if not assetList or len(assetList) == 0:\n",
    "        raise IndexError(f'asset list is empty or not defined')\n",
    "        \n",
    "    \n",
    "    dataAssets = []    \n",
    "    \n",
    "    if sync:\n",
    "        newDatasetList = [asset[f'{fromEnv}Id'] for asset in assetList if asset['type'] == 'dataset']\n",
    "        dataAssets = getAssetList(fromEnv, newDatasetList)\n",
    "\n",
    "    else:   \n",
    "        dataAssets = getAssetList(fromEnv, assetList)\n",
    "    \n",
    "    try:\n",
    "        print(f'{bcolors.OKBLUE}Preparing to {\"sync\" if sync else \"copy\"} from {fromEnv} to {toEnv}...{bcolors.ENDC}')\n",
    "        resources = []\n",
    "        \n",
    "        # @TODO:\n",
    "        # Improve loop performance with multiprocessing\n",
    "        # move loops into reusable function based on type\n",
    "        # For sync only path updated data\n",
    "        \n",
    "        for dataset in dataAssets['data']:\n",
    "            toDatasetId = assetIdToBeSync(sync, assetList, dataset, fromEnv, toEnv)\n",
    "            if toDatasetId:\n",
    "                logger.info(f'sync [{fromEnv}]dataset: {dataset.get(\"id\")}')\n",
    "                logger.info(f'with [{toEnv}]dataset: {toDatasetId}')\n",
    "            newDataset = recreateDataset(dataset, toEnv, toDatasetId)\n",
    "\n",
    "            resources.append({\n",
    "                'type': 'dataset',\n",
    "                f'{fromEnv}Id':dataset.get('id'),\n",
    "                f'{toEnv}Id': newDataset['data'].get('id')\n",
    "            })\n",
    "\n",
    "            for vocabulary in dataset['attributes'].get('vocabulary'):\n",
    "                newVocabulary = recreateVocabulary(newDataset['data'].get('id'), vocabulary, toEnv)\n",
    "                \n",
    "                resources.append({\n",
    "                'type': 'vocabulary',\n",
    "                f'{fromEnv}Id':vocabulary.get('id'),\n",
    "                f'{toEnv}Id': newVocabulary['data']\n",
    "            })\n",
    "\n",
    "            # sync layers\n",
    "            for layer in dataset['attributes'].get('layer'):\n",
    "                \n",
    "                toLayerId = assetIdToBeSync(sync, assetList, layer, fromEnv, toEnv)\n",
    "                if toLayerId:\n",
    "                    logger.info(f'sync [{fromEnv}]layer: {layer.get(\"id\")}')\n",
    "                    logger.info(f'with [{toEnv}]layer: {toLayerId}')\n",
    "                \n",
    "                newLayer = recreateLayer(newDataset['data'].get('id'), layer, toEnv, toLayerId)\n",
    "                \n",
    "                resources.append({\n",
    "                'type': 'layer',\n",
    "                f'{fromEnv}Id':layer.get('id'),\n",
    "                f'{toEnv}Id': newLayer['data'].get('id')\n",
    "                })\n",
    "                \n",
    "                fromLayerMetadata = getSubAssetMetadata(dataset.get(\"id\"), layerId=layer.get(\"id\"), fromEnv=fromEnv)\n",
    "                \n",
    "                if fromLayerMetadata:\n",
    "                    for layerMetadata in fromLayerMetadata.get('data', {}):\n",
    "                        logger.info('creating metadata for layer...')\n",
    "                        newMetadata = recreateMetadata(newDataset['data'].get('id'), layerMetadata, layerId=newLayer['data'].get('id'), toEnv=toEnv)\n",
    "                        \n",
    "                        resources.append({\n",
    "                        'type': 'metadata',\n",
    "                        f'{fromEnv}Id':layerMetadata.get('id'),\n",
    "                        f'{toEnv}Id': newMetadata['data']\n",
    "                        })   \n",
    "            \n",
    "            # remove toEnv layers that are not on fromEnv            \n",
    "            for layer in getAssetList(toEnv, [toDatasetId])['data'][0]['attributes'].get('layer'):\n",
    "                if layer.get(\"id\") not in [asset[f'{toEnv}Id'] for asset in resources if asset['type'] == 'layer']:\n",
    "                    headers = setTokenHeader(toEnv)\n",
    "                    serverUrl = {\n",
    "                        'prod': prod_server,\n",
    "                        'staging': staging_server\n",
    "                    }\n",
    "                    url = f'{serverUrl[toEnv]}/v1/dataset/{toDatasetId}/layer/{layer.get(\"id\")}'\n",
    "                    deleteAssets(url, headers)   \n",
    "            \n",
    "            # sync widgets\n",
    "            for widget in dataset['attributes'].get('widget'):\n",
    "                \n",
    "                toWidgetId = assetIdToBeSync(sync, assetList, widget, fromEnv, toEnv)\n",
    "                if toWidgetId:\n",
    "                    logger.info(f'sync [{fromEnv}]widget: {widget.get(\"id\")}')\n",
    "                    logger.info(f'with [{toEnv}]widget: {toWidgetId}')\n",
    "                    \n",
    "                newWidget = recreateWidget(newDataset['data'].get('id'), widget, toEnv, toWidgetId)                \n",
    "                resources.append({\n",
    "                'type': 'widget',\n",
    "                f'{fromEnv}Id':widget.get('id'),\n",
    "                f'{toEnv}Id': newWidget['data'].get('id')\n",
    "                })\n",
    "                \n",
    "                fromWidgetMetadata = getSubAssetMetadata(dataset.get(\"id\"), widgetId=widget.get(\"id\"), fromEnv=fromEnv)\n",
    "                \n",
    "                if fromWidgetMetadata:\n",
    "                    for widgetMetadata in fromWidgetMetadata.get('data', {}):\n",
    "                        logger.info('creating metadata for widget...')\n",
    "                        newMetadata = recreateMetadata(newDataset['data'].get('id'), widgetMetadata, widgetId=newWidget['data'].get('id'), toEnv=toEnv)\n",
    "                        \n",
    "                        resources.append({\n",
    "                        'type': 'metadata',\n",
    "                        f'{fromEnv}Id':widgetMetadata.get('id'),\n",
    "                        f'{toEnv}Id': newMetadata['data']\n",
    "                        })\n",
    "\n",
    "            # remove toEnv widgets that are not on fromEnv            \n",
    "            for widget in getAssetList(toEnv, [toDatasetId])['data'][0]['attributes'].get('widget'):\n",
    "                if widget.get(\"id\") not in [asset[f'{toEnv}Id'] for asset in resources if asset['type'] == 'widget']:\n",
    "                    headers = setTokenHeader(toEnv)\n",
    "                    serverUrl = {\n",
    "                        'prod': prod_server,\n",
    "                        'staging': staging_server\n",
    "                    }\n",
    "                    url = f'{serverUrl[toEnv]}/v1/dataset/{toDatasetId}/widget/{widget.get(\"id\")}'\n",
    "                    deleteAssets(url, headers)       \n",
    "\n",
    "            for metadata in dataset['attributes'].get('metadata'):\n",
    "                logger.info('creating metadata')\n",
    "                newMetadata = recreateMetadata(newDataset['data'].get('id'), metadata, toEnv=toEnv)\n",
    "                \n",
    "                resources.append({\n",
    "                'type': 'metadata',\n",
    "                f'{fromEnv}Id':metadata.get('id'),\n",
    "                f'{toEnv}Id': newMetadata['data']\n",
    "                })\n",
    "            \n",
    "    except NameError or IndexError as e:\n",
    "        logger.error(e)\n",
    "        raise e\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    print(f'{bcolors.OKGREEN}{\"sync\" if sync else \"copy\"} process finished{bcolors.ENDC}')\n",
    "\n",
    "    filename = f'dataset_sync_files/RW_prod_staging_match_{resources[0][\"prodId\"]}.json'\n",
    "    if not sync and len(resources) > 0:\n",
    "        print(f'creating sync file with name: {filename}')\n",
    "    elif sync and len(resources) > 0:\n",
    "        print(f'update sync file {filename}')\n",
    "    with open(filename, 'w') as outfile:\n",
    "        json.dump(resources, outfile)\n",
    "    return filename\n",
    "\n",
    "    \n",
    "    \n",
    "        \n",
    "def syncAssets(syncList, fromEnv='prod', toEnv='staging'):\n",
    "    '''\n",
    "    Allows sync of Assets\n",
    "    '''\n",
    "    \n",
    "    return copyAssets(syncList, True, fromEnv, toEnv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing\n",
    "## Get list of assets that we want to modify or sync"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List of assets:\n",
    "\n",
    "* `datasetsProd` will contain the id of the assets in productioon that need to be migrated to `staging`. We need to make sure that this list is in sync with the document we have shared with the assets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For testing purposes\n",
    "Dummy assests to create assets in production environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': {'id': '6a3aa408-b3d3-44c6-89b7-93fbfa545489',\n",
       "  'type': 'dataset',\n",
       "  'attributes': {'name': 'This is a test',\n",
       "   'slug': 'This-is-a-test_9',\n",
       "   'type': None,\n",
       "   'subtitle': None,\n",
       "   'application': ['rw'],\n",
       "   'dataPath': None,\n",
       "   'attributesPath': None,\n",
       "   'connectorType': 'rest',\n",
       "   'provider': 'cartodb',\n",
       "   'userId': '57a0aa1071e394dd32ffe137',\n",
       "   'connectorUrl': 'https://wri-rw.carto.com/api/v2/sql?q=select * from air_temo_anomalies',\n",
       "   'sources': [],\n",
       "   'tableName': 'air_temo_anomalies',\n",
       "   'status': 'pending',\n",
       "   'published': False,\n",
       "   'overwrite': False,\n",
       "   'mainDateField': None,\n",
       "   'env': 'production',\n",
       "   'geoInfo': False,\n",
       "   'protected': False,\n",
       "   'legend': {'date': [],\n",
       "    'region': [],\n",
       "    'country': [],\n",
       "    'nested': [],\n",
       "    'integer': [],\n",
       "    'short': [],\n",
       "    'byte': [],\n",
       "    'double': [],\n",
       "    'float': [],\n",
       "    'half_float': [],\n",
       "    'scaled_float': [],\n",
       "    'boolean': [],\n",
       "    'binary': [],\n",
       "    'text': [],\n",
       "    'keyword': []},\n",
       "   'clonedHost': {},\n",
       "   'errorMessage': None,\n",
       "   'taskId': None,\n",
       "   'createdAt': '2021-06-07T09:36:12.332Z',\n",
       "   'updatedAt': '2021-06-07T09:36:12.332Z',\n",
       "   'dataLastUpdated': None,\n",
       "   'widgetRelevantProps': [],\n",
       "   'layerRelevantProps': []}}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dummy data to test the notebook: creation of a dummy dataset with a layer in production.\n",
    "toEnv = 'prod'\n",
    "serverUrl = {\n",
    "        'prod': prod_server,\n",
    "        'staging': staging_server\n",
    "    }\n",
    "headers = setTokenHeader(toEnv)\n",
    "urlDataset = f'{serverUrl[toEnv]}/v1/dataset'\n",
    "bodyDataset = {'dataset':{\n",
    "    'application': ['rw'],\n",
    "    'name': 'This is a test',\n",
    "    'connectorType': 'rest',\n",
    "    'provider': 'cartodb',\n",
    "    'published': False,\n",
    "    'overwrite': False,\n",
    "    'protected':False,\n",
    "    'env': 'production',\n",
    "    'connectorUrl': \"https://wri-rw.carto.com/api/v2/sql?q=select * from air_temo_anomalies\"\n",
    "    }\n",
    "}\n",
    "\n",
    "responseDataset = postAssets(urlDataset, bodyDataset, headers)\n",
    "responseDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': {'id': 'c21dd7ab-e729-4811-9433-8333b1d7c9e9',\n",
       "  'type': 'layer',\n",
       "  'attributes': {'name': 'test-121',\n",
       "   'slug': 'test-121_2',\n",
       "   'dataset': '6a3aa408-b3d3-44c6-89b7-93fbfa545489',\n",
       "   'application': ['rw'],\n",
       "   'iso': [],\n",
       "   'provider': 'cartodb',\n",
       "   'userId': '57a0aa1071e394dd32ffe137',\n",
       "   'default': True,\n",
       "   'protected': False,\n",
       "   'published': False,\n",
       "   'env': 'production',\n",
       "   'layerConfig': {'body': {}},\n",
       "   'legendConfig': {},\n",
       "   'interactionConfig': {},\n",
       "   'applicationConfig': {},\n",
       "   'staticImageConfig': {},\n",
       "   'createdAt': '2021-06-07T09:36:15.327Z',\n",
       "   'updatedAt': '2021-06-07T09:36:15.327Z'}}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urlLayer = f'{urlDataset}/{responseDataset[\"data\"].get(\"id\")}/layer'\n",
    "bodyLayer = {\n",
    "        'application': ['rw'],\n",
    "        'name': 'test-121',\n",
    "        'provider': 'cartodb',\n",
    "        'default': True,\n",
    "        'published': False,\n",
    "        'env': 'production',\n",
    "        'layerConfig': {\n",
    "            \"body\": {}\n",
    "            },\n",
    "        'legendConfig': {},\n",
    "        'interactionConfig': {},\n",
    "        'applicationConfig': {}\n",
    "    }\n",
    "responseLayer = postAssets(urlLayer, bodyLayer, headers)\n",
    "responseLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': {'id': '5f169df0-a293-4588-bbcd-521ee9484cd6',\n",
       "  'type': 'widget',\n",
       "  'attributes': {'name': 'test-121',\n",
       "   'dataset': '6a3aa408-b3d3-44c6-89b7-93fbfa545489',\n",
       "   'slug': 'test-121_2',\n",
       "   'userId': '57a0aa1071e394dd32ffe137',\n",
       "   'application': ['rw'],\n",
       "   'verified': False,\n",
       "   'default': True,\n",
       "   'protected': False,\n",
       "   'defaultEditableWidget': False,\n",
       "   'published': False,\n",
       "   'freeze': False,\n",
       "   'env': 'production',\n",
       "   'widgetConfig': {'body': {}},\n",
       "   'template': False,\n",
       "   'createdAt': '2021-06-07T09:36:17.153Z',\n",
       "   'updatedAt': '2021-06-07T09:36:17.154Z'}}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urlWidget = f'{urlDataset}/{responseDataset[\"data\"].get(\"id\")}/widget'\n",
    "bodyWidget = {\n",
    "        'application': ['rw'],\n",
    "        'name': 'test-121',\n",
    "        'default': True,\n",
    "        'published': False,\n",
    "        'env': 'production',\n",
    "        'widgetConfig': {\n",
    "            \"body\": {}\n",
    "            }\n",
    "    }\n",
    "responseWidget = postAssets(urlWidget, bodyWidget, headers)\n",
    "responseWidget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': [{'id': 'knowledge_graph',\n",
       "   'type': 'vocabulary',\n",
       "   'attributes': {'tags': ['geospatial'],\n",
       "    'name': 'knowledge_graph',\n",
       "    'application': 'rw'}}]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urlVocabulary = f'{urlDataset}/{responseDataset[\"data\"].get(\"id\")}/vocabulary/knowledge_graph'\n",
    "bodyVocabulary = {\n",
    "        'application': 'rw',\n",
    "        'tags':[\"geospatial\"]\n",
    "    }\n",
    "responseVocabulary = postAssets(urlVocabulary, bodyVocabulary, headers)\n",
    "responseVocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': [{'id': '60bde8962852be001ba7e42b',\n",
       "   'type': 'metadata',\n",
       "   'attributes': {'dataset': '6a3aa408-b3d3-44c6-89b7-93fbfa545489',\n",
       "    'application': 'rw',\n",
       "    'resource': {'id': '6a3aa408-b3d3-44c6-89b7-93fbfa545489',\n",
       "     'type': 'dataset'},\n",
       "    'language': 'eng',\n",
       "    'name': 'this is a dummy dataset',\n",
       "    'description': 'Lorem Ipsum',\n",
       "    'createdAt': '2021-06-07T09:36:22.304Z',\n",
       "    'updatedAt': '2021-06-07T09:36:22.304Z',\n",
       "    'status': 'published'}}]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urlMetadataDataset = f'{urlDataset}/{responseDataset[\"data\"].get(\"id\")}/metadata'\n",
    "bodyMetadataDataset = {\n",
    "        'application': 'rw',\n",
    "        'language':'ENG',\n",
    "        'name':'this is a dummy dataset',\n",
    "        'description':'Lorem Ipsum'\n",
    "    }\n",
    "responseMetadataDataset = postAssets(urlMetadataDataset, bodyMetadataDataset, headers)\n",
    "responseMetadataDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urlMetadataLayer = f'{urlDataset}/{responseDataset[\"data\"].get(\"id\")}/layer/{responseLayer[\"data\"].get(\"id\")}/metadata'\n",
    "bodyMetadataLayer = {\n",
    "        'application': 'rw',\n",
    "        'language':'ENG',\n",
    "        'name':'this is a dummy Layer',\n",
    "        'description':'Lorem Ipsum'\n",
    "    }\n",
    "responseMetadataLayer = postAssets(urlMetadataLayer, bodyMetadataLayer, headers)\n",
    "responseMetadataLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': [{'id': '60bde89a3cc064001b3675b9',\n",
       "   'type': 'metadata',\n",
       "   'attributes': {'dataset': '6a3aa408-b3d3-44c6-89b7-93fbfa545489',\n",
       "    'application': 'rw',\n",
       "    'resource': {'id': '5f169df0-a293-4588-bbcd-521ee9484cd6',\n",
       "     'type': 'widget'},\n",
       "    'language': 'eng',\n",
       "    'name': 'this is a dummy widget',\n",
       "    'description': 'Lorem Ipsum',\n",
       "    'createdAt': '2021-06-07T09:36:26.194Z',\n",
       "    'updatedAt': '2021-06-07T09:36:26.194Z',\n",
       "    'status': 'published'}}]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urlMetadatawidget = f'{urlDataset}/{responseDataset[\"data\"].get(\"id\")}/widget/{responseWidget[\"data\"].get(\"id\")}/metadata'\n",
    "bodyMetadatawidget = {\n",
    "        'application': 'rw',\n",
    "        'language':'ENG',\n",
    "        'name':'this is a dummy widget',\n",
    "        'description':'Lorem Ipsum'\n",
    "    }\n",
    "responseMetadatawidget = postAssets(urlMetadatawidget, bodyMetadatawidget, headers)\n",
    "responseMetadatawidget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List of assets:\n",
    "\n",
    "* we need to make sure that this list is in sync with the document we have shared with the assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['6a3aa408-b3d3-44c6-89b7-93fbfa545489']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# in the future we can automate this listing based on the doc using the google sheet api both for writing and reading from\n",
    "# providing a sample of the list by printing it\n",
    "datasetsProd = [responseDataset['data']['id']]\n",
    "datasetsProd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backup Data in both environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#backupAssets('prod')\n",
    "#backupAssets('staging')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only do this if you want to clean data in staging. \n",
    "* You will need to be logged in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#deleteDataFrom()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy resources from production to staging. \n",
    "The running time will depend on the size of the asset.   \n",
    "Running this cell is only needed to create new assets from `production` to `staging`.\n",
    "A json file is created with a unique name in local. The json files contains for each assest:\n",
    "- type: this can be a \"layer\", a \"dataset\", a \"widget\", \"vocabulary\", \"metadata\"\n",
    "- prodId: the id of the item in `production`\n",
    "- stagingId: the id of the item in `staging`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter the API ID of the dataset on production to copy/sync here\n",
    "prod_API_ID = ['']# ex: '79e06dd8-a2ae-45eb-8e99-e73bc87ec946'\n",
    "# keep the syncFile list empty\n",
    "syncFile = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:root:sync [prod]dataset: 104c163c-dbd5-4660-b1ce-b11039933abb\n",
      "INFO:root:with [staging]dataset: 3084cc07-9ee2-4b53-8125-ef11174f6296\n",
      "\u001b[94mPreparing to sync from prod to staging...\u001b[0m\n",
      "INFO:root:sync [prod]layer: a34942b8-1719-4aa1-b221-1c6602121207\n",
      "INFO:root:with [staging]layer: ba3d4161-6058-4af5-81fb-edc961cc3fff\n",
      "INFO:root:sync [prod]layer: 7d05a424-7d01-47d9-b77c-d084828497e0\n",
      "INFO:root:with [staging]layer: dfa99bf1-8b56-4574-ad51-01dd6fedd227\n",
      "INFO:root:sync [prod]layer: 9f326ff0-f9fc-496e-83d8-b48eea7f3966\n",
      "INFO:root:with [staging]layer: 73c8fb63-3570-45e4-a6aa-60b45f83c48a\n",
      "INFO:root:sync [prod]layer: a5c6eb9a-1c58-4355-a6e8-ca53c930fa48\n",
      "INFO:root:with [staging]layer: a8428ca8-8d04-4c65-b266-ff2c62dddd37\n",
      "INFO:root:sync [prod]layer: 3fa1d13a-8114-4b29-81b9-b628cc0f0eaa\n",
      "INFO:root:with [staging]layer: f8d12be8-d656-47a7-ae56-699f850e27ed\n",
      "INFO:root:creating metadata for widget...\n",
      "INFO:root:creating metadata\n",
      "\u001b[92msync process finished\u001b[0m\n",
      "update sync file dataset_sync_files/RW_prod_staging_match_104c163c-dbd5-4660-b1ce-b11039933abb.json\n",
      "INFO:root:sync [prod]dataset: 3db2f914-2c70-431c-8dce-5dd961bccbd5\n",
      "INFO:root:with [staging]dataset: cedd24b3-51aa-46c3-8d22-7d14717d25ab\n",
      "\u001b[94mPreparing to sync from prod to staging...\u001b[0m\n",
      "ERROR:root:response: \n",
      "ERROR:root:<Response [400]>\n",
      "ERROR:root:url: \n",
      "ERROR:root:https://staging-api.resourcewatch.org/v1/dataset/cedd24b3-51aa-46c3-8d22-7d14717d25ab/vocabulary/knowledge_graph\n",
      "ERROR:root:body: \n",
      "ERROR:root:{\"application\": \"rw\", \"tags\": [\"health\", \"disease\", \"geospatial\", \"raster\", \"society\", \"annual\", \"historical\", \"global\"]}\n",
      "\u001b[93mPost operation was not succesfull, trying to update instead\u001b[0m\n",
      "INFO:root:sync [prod]layer: 833d8248-5573-4677-acf5-62b45b8e248a\n",
      "INFO:root:with [staging]layer: 8c357c89-2b46-4da6-966d-a8f79df9e482\n",
      "INFO:root:sync [prod]layer: e5c31a54-68ca-42a4-a93e-41bdd399b503\n",
      "INFO:root:with [staging]layer: 81487b11-cf84-4a84-a45a-3f2b93aca72c\n",
      "INFO:root:sync [prod]layer: 69b5aa54-7da1-4a3f-9416-937964455f97\n",
      "INFO:root:with [staging]layer: c2e3755e-cae1-488b-bc43-5c0e61dd7bae\n",
      "INFO:root:sync [prod]layer: f35b293c-2eaf-4f73-a49c-ca2d8af4f4cc\n",
      "INFO:root:with [staging]layer: ff195640-baba-43f8-9a65-9d3303965969\n",
      "INFO:root:sync [prod]layer: 3c55834e-7eb4-4b8c-b503-034baf197761\n",
      "INFO:root:with [staging]layer: 89291444-ca2b-4067-aecb-41f4ddabe940\n",
      "INFO:root:sync [prod]layer: a1db0679-9edb-4179-9d4d-af275da4e4be\n",
      "INFO:root:with [staging]layer: ba40bf20-1320-4d59-9b91-2b4c1d599e91\n",
      "INFO:root:sync [prod]layer: 879e3fe6-1e88-4193-91ef-6d248c015272\n",
      "INFO:root:with [staging]layer: 04d96bbe-c354-450e-9721-def5d764ff76\n",
      "INFO:root:sync [prod]layer: 2ee5877c-b582-4968-a033-c404a68e7912\n",
      "INFO:root:with [staging]layer: 6376633a-2cc1-4085-9b7f-3c5ad802548c\n",
      "INFO:root:sync [prod]layer: 5b8d15fe-dd01-4f58-b872-d57204505756\n",
      "INFO:root:with [staging]layer: 626008d5-47ea-4677-a643-3d31a512acc6\n",
      "INFO:root:sync [prod]layer: d2977700-9b77-4769-b868-af732e4f242e\n",
      "INFO:root:with [staging]layer: 8daa3822-d607-4f00-90cd-c845e96e6022\n",
      "INFO:root:sync [prod]layer: fca91587-a696-431a-906a-4808cafab071\n",
      "INFO:root:with [staging]layer: 155a8ff2-5393-4ca2-9182-ece0936d75a4\n",
      "INFO:root:sync [prod]layer: 8917bafb-aa90-4ba6-b4ca-cd5df7bc8e00\n",
      "INFO:root:with [staging]layer: 5be87767-17d7-4296-8532-b6c11c356311\n",
      "INFO:root:sync [prod]layer: 4351526b-3f1e-4a6c-80e3-d8f90b97af6e\n",
      "INFO:root:with [staging]layer: 7d317ddd-eb1c-4a3a-ba38-42159c4a90ed\n",
      "INFO:root:sync [prod]layer: 5f231e62-e0f1-49e1-9c1e-8c4afc88b751\n",
      "INFO:root:with [staging]layer: 544faf0f-6869-4479-9084-282ac81da54b\n",
      "INFO:root:sync [prod]layer: 09757b61-d191-485b-968f-4410e910e502\n",
      "INFO:root:with [staging]layer: 8f430cc2-88f2-4f8b-9e40-32535ba50089\n",
      "INFO:root:sync [prod]layer: 6e054cb2-96db-43e6-8888-7b3d02a6570f\n",
      "INFO:root:with [staging]layer: 4cfa2ffc-7930-4792-94c7-e135f2cf3dc8\n",
      "INFO:root:sync [prod]layer: 9bdf4250-3a34-4623-9245-0fd9d430c408\n",
      "INFO:root:with [staging]layer: 5c88f0c5-c48a-4a09-b1f0-07f6b9108133\n",
      "INFO:root:sync [prod]layer: 8535310c-d213-4b69-92b0-8027ae625434\n",
      "INFO:root:with [staging]layer: f9509a0a-5733-403d-911a-2b2b7b87a4c9\n",
      "INFO:root:creating metadata for widget...\n",
      "INFO:root:creating metadata for widget...\n",
      "INFO:root:creating metadata\n",
      "\u001b[92msync process finished\u001b[0m\n",
      "update sync file dataset_sync_files/RW_prod_staging_match_3db2f914-2c70-431c-8dce-5dd961bccbd5.json\n"
     ]
    }
   ],
   "source": [
    "# copy a dataset on production to staging\n",
    "for datasetId in prod_API_ID:\n",
    "    syncFile.append(copyAssets([datasetId], False, fromEnv='prod', toEnv = 'staging'))\n",
    "for syncfile in syncFile:\n",
    "    with open(syncfile) as json_file:\n",
    "        syncList = json.load(json_file)\n",
    "    syncAssets(syncList, fromEnv='prod', toEnv='staging')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open sync list of assets, match items with list and update them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[94mPreparing to sync from prod to staging...\u001b[0m\n",
      "\u001b[92msync process finished\u001b[0m\n",
      "update sync file dataset_sync_files/RW_prod_staging_match_c88d0058-3af3-4034-9915-1c8b336cfb62.json\n"
     ]
    }
   ],
   "source": [
    "# sync dataset production <> staging\n",
    "# use the printed json filename in the previous cell\n",
    "if len(syncFile)==0:\n",
    "    syncFile = [f'dataset_sync_files/RW_prod_staging_match_{datasetId}.json' for datasetId in prod_API_ID]\n",
    "for syncfile in syncFile:\n",
    "    with open(syncfile) as json_file:\n",
    "        syncList = json.load(json_file)\n",
    "\n",
    "    syncAssets(syncList, fromEnv='prod', toEnv='staging')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\u001b[93mAre you sure you want to delete         ['6a3aa408-b3d3-44c6-89b7-93fbfa545489'] in prod:\u001b[0m         Y/n Y\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:deleting https://api.resourcewatch.org/v1/dataset/6a3aa408-b3d3-44c6-89b7-93fbfa545489... \n"
     ]
    }
   ],
   "source": [
    "# delete testing datasets from both envs after testing:\n",
    "deleteDataFrom('prod', [responseDataset['data']['id']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\u001b[93mAre you sure you want to delete         ['41e6b9c7-481c-403c-a609-82adc3e59000'] in staging:\u001b[0m         Y/n Y\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:deleting https://staging-api.globalforestwatch.org/v1/dataset/41e6b9c7-481c-403c-a609-82adc3e59000... \n"
     ]
    }
   ],
   "source": [
    "deleteDataFrom('staging', [syncList[0]['stagingId']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.7 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "interpreter": {
   "hash": "1936b053440c27f530542f53326030d97194af8aa0e5ac751988556632f9c990"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}