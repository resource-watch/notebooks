{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Migration and sync of assets between prod and staging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently the production API is the one that has the latest updated data by the WRI team. \n",
    "This notebook copies assets from `production` to `staging` maintening the match between IDs. Optionally, it would be possible to copy assets back from `staging` to `production`. \n",
    "\n",
    "### Steps:\n",
    "1. upload/update assest to `production`\n",
    "2. make a copy of the assests from `production` to `staging` using this script\n",
    "3. synchronise the ids of the assets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "1. run the `Functions`.\n",
    "2. create a list with the assets urls to copy.\n",
    "3. `Processing` has the steps to carry out the migration. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions\n",
    "These are the functions we need to create and synchronise assets from `staging` to `production`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "import requests as re\n",
    "import json\n",
    "from datetime import datetime\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "staging_server = \"https://staging-api.resourcewatch.org\"\n",
    "prod_server = \"https://api.resourcewatch.org\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class bcolors:\n",
    "    HEADER = '\\033[95m'\n",
    "    OKBLUE = '\\033[94m'\n",
    "    OKCYAN = '\\033[96m'\n",
    "    OKGREEN = '\\033[92m'\n",
    "    WARNING = '\\033[93m'\n",
    "    FAIL = '\\033[91m'\n",
    "    ENDC = '\\033[0m'\n",
    "    BOLD = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auth(env='prod'):\n",
    "    serverUrl = {\n",
    "        'prod': prod_server,\n",
    "        'staging': staging_server\n",
    "    }\n",
    "    print(f'You are login into {bcolors.HEADER}{bcolors.BOLD}{env}{bcolors.ENDC}')\n",
    "    with re.Session() as s:\n",
    "        headers = {'Content-Type': 'application/json'}\n",
    "        payload = json.dumps({ 'email': f'{input(f\"Email: \")}',\n",
    "                               'password': f'{getpass.getpass(prompt=\"Password: \")}'})\n",
    "        response = s.post(f'{serverUrl[env]}/auth/login',  headers = headers,  data = payload)\n",
    "        response.raise_for_status()\n",
    "        print(f'{bcolors.OKGREEN}Successfully logged into {env}{bcolors.ENDC}')\n",
    "    return response.json().get('data').get('token')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "You are login into \u001b[95m\u001b[1mstaging\u001b[0m\n",
      "\u001b[92mSuccessfully logged into staging\u001b[0m\n",
      "You are login into \u001b[95m\u001b[1mprod\u001b[0m\n",
      "\u001b[92mSuccessfully logged into prod\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "token = {\n",
    "    'staging': auth('staging'),\n",
    "    'prod':auth('prod')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @TODO \n",
    "# * Migrate one day the body payloads to data model classes and refactor to classes following inheritance and recursive property copies\n",
    "# * Type function with Mypy\n",
    "# * Add proper method descriptions\n",
    "# * Refactor methods to reuse more code\n",
    "#from typing import List\n",
    "#from pydantic import BaseModel, parse_obj_as\n",
    "# class DatasetModel(BaseModel):\n",
    "\n",
    "# class LayerModel(BaseModel):\n",
    "\n",
    "# class widgetModel(BaseModel):\n",
    "\n",
    "# class metadataModel(BaseModel):\n",
    "     \n",
    "# class vocabularyModel(BaseModel):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setTokenHeader(env, token=token):\n",
    "    '''\n",
    "    set up the token\n",
    "    '''\n",
    "    return {'Authorization':f'Bearer {token[env]}', \n",
    "            'Content-Type': 'application/json'}\n",
    "\n",
    "def logResponseErrors(status_code, response = None, url = None, body = None):\n",
    "    '''\n",
    "    log errors in http calls\n",
    "    '''\n",
    "    if status_code !=200:\n",
    "        logging.error('response: ')\n",
    "        logging.error(response)\n",
    "        logging.error(response.text) if response else None\n",
    "        logging.error(response.json()) if response else None\n",
    "        logging.error('url: ')\n",
    "        logging.error(url) if url else None\n",
    "        logging.error('body: ')\n",
    "        logging.error(json.dumps(body)) if body else None\n",
    "    \n",
    "\n",
    "def getAssets(url, payload=None):\n",
    "    '''\n",
    "    Get asset operation\n",
    "    '''\n",
    "    response = re.get(url, payload)\n",
    "    \n",
    "    logResponseErrors(response.status_code, response, url, payload)\n",
    "    \n",
    "    response.raise_for_status()\n",
    "    \n",
    "    return response.json()\n",
    "\n",
    "def deleteAssets(url, headers):\n",
    "    '''\n",
    "    delete asset operation\n",
    "    '''\n",
    "    response = re.delete(url, headers = headers)\n",
    "    \n",
    "    logResponseErrors(response.status_code, response, url)\n",
    "    \n",
    "    response.raise_for_status()\n",
    "    \n",
    "    return response.status_code\n",
    "\n",
    "def postAssets(url, body, headers, payloads = None):\n",
    "    '''\n",
    "    create asset operation\n",
    "    '''    \n",
    "    response = re.post(url, params = payloads, data=json.dumps(body), headers = headers)\n",
    "    \n",
    "    logResponseErrors(response.status_code, response, url, body)\n",
    "    \n",
    "    response.raise_for_status()\n",
    "    \n",
    "    return response.json()\n",
    "\n",
    "def updateAssets(url, body, headers):\n",
    "    '''\n",
    "    patch asset operation\n",
    "    '''\n",
    "    response = re.patch(url, data=json.dumps(body), headers = headers)\n",
    "    \n",
    "    logResponseErrors(response.status_code, response, url, body)\n",
    "    \n",
    "    response.raise_for_status()\n",
    "    \n",
    "    return response.json()\n",
    "\n",
    "def copyAssetBody(asset, excludeList=['createdAt', 'updatedAt','clonedHost', 'errorMessage', 'taskId', 'status', 'sources',\n",
    "                                      'userId', 'slug', 'dataset', 'layer', 'widget', 'metadata', 'vocabulary']):\n",
    "    '''\n",
    "    Copy a body dict to a new dict excluding some keys or not defined values.\n",
    "    '''\n",
    "    response = {}\n",
    "    response.update(asset)\n",
    "    \n",
    "    for key, value in asset.items():\n",
    "        if (key in excludeList or value is None or (type(value) == dict and len(value) == 0) ):\n",
    "            response.pop(key, None)\n",
    "    \n",
    "    if 'provider' in response.keys() and response['provider'] =='cartodb':\n",
    "        response.pop('tableName', None)\n",
    "    \n",
    "    return response\n",
    "\n",
    "def upsert(conditon = False):\n",
    "    '''\n",
    "    Return an update/post operation base on a condition\n",
    "    '''\n",
    "    if conditon:\n",
    "        return updateAssets\n",
    "    else:\n",
    "        return postAssets\n",
    "    \n",
    "def recreateDataset(dataset, toEnv = 'prod', destinationDatasetId = None):\n",
    "    '''\n",
    "    Copy the dataset from one env to the other\n",
    "    '''\n",
    "    \n",
    "    serverUrl = {\n",
    "        'prod': prod_server,\n",
    "        'staging': staging_server\n",
    "    }\n",
    "    if dataset.get('type')!='dataset':\n",
    "        return None\n",
    "    \n",
    "    url = f'{serverUrl[toEnv]}/v1/dataset'\n",
    "    \n",
    "    if destinationDatasetId:\n",
    "        url = f'{url}/{destinationDatasetId}' \n",
    "        \n",
    "    body = {'dataset': copyAssetBody(dataset.get('attributes'))}\n",
    "    \n",
    "    logger.debug(body)\n",
    "    \n",
    "    headers = setTokenHeader(toEnv)\n",
    "    \n",
    "    response = upsert(destinationDatasetId)\n",
    "    \n",
    "    logger.debug(response)\n",
    "    if destinationDatasetId:       \n",
    "        return response(url, body['dataset'], headers)\n",
    "    else:\n",
    "        return response(url, body, headers)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "def recreateLayer(datasetId, layer, toEnv = 'prod', destinationLayerId = None):\n",
    "    '''\n",
    "    Copy the layer from one env to the other\n",
    "    '''\n",
    "    \n",
    "    serverUrl = {\n",
    "        'prod': prod_server,\n",
    "        'staging': staging_server\n",
    "    }\n",
    "    if layer.get('type')!='layer':\n",
    "        return None\n",
    "    \n",
    "    headers = setTokenHeader(toEnv)\n",
    "    url = f'{serverUrl[toEnv]}/v1/dataset/{datasetId}/layer'\n",
    "    \n",
    "    if destinationLayerId:\n",
    "        url = f'{url}/{destinationLayerId}'\n",
    "\n",
    "    body = copyAssetBody(layer.get('attributes'))\n",
    "    \n",
    "    response = upsert(destinationLayerId)\n",
    "    \n",
    "    return response(url, body, headers)\n",
    "\n",
    "def recreateWidget(datasetId, widget, toEnv = 'prod', destinationWidgetId = None):\n",
    "    '''\n",
    "    Copy the widget from one env to the other\n",
    "    '''\n",
    "    \n",
    "    serverUrl = {\n",
    "        'prod': prod_server,\n",
    "        'staging': staging_server\n",
    "    }\n",
    "    \n",
    "    if widget.get('type')!='widget':\n",
    "        return None\n",
    "    \n",
    "    headers = setTokenHeader(toEnv)\n",
    "    url = f'{serverUrl[toEnv]}/v1/dataset/{datasetId}/widget'\n",
    "    \n",
    "    if destinationWidgetId:\n",
    "        url = f'{url}/{destinationWidgetId}'\n",
    "    \n",
    "    body = copyAssetBody(widget.get('attributes'))\n",
    "    \n",
    "    response = upsert(destinationWidgetId)\n",
    "    \n",
    "    return response(url, body, headers)\n",
    "\n",
    "def getSubAssetMetadata(datasetId, layerId=None, widgetId=None, fromEnv = 'prod'):\n",
    "    '''\n",
    "    Get metadata that is not given back from main call\n",
    "    '''\n",
    "    serverUrl = {\n",
    "        'prod': prod_server,\n",
    "        'staging': staging_server\n",
    "    }\n",
    "    \n",
    "    if (layerId and widgetId) or (not layerId and not widgetId):\n",
    "        raise Exception(\"layerId and widgetId not allowed at the same time\")\n",
    "    elif layerId:\n",
    "        url = f'{serverUrl[fromEnv]}/v1/dataset/{datasetId}/layer/{layerId}/metadata'\n",
    "    elif widgetId:\n",
    "        url = f'{serverUrl[fromEnv]}/v1/dataset/{datasetId}/widget/{widgetId}/metadata'\n",
    "        \n",
    "    try:\n",
    "        return getAssets(url)\n",
    "    except Exception as e:\n",
    "        logger.info('Get operation was not successfull')\n",
    "        logger.error(f'{e}')\n",
    "        return None\n",
    "        pass\n",
    "    \n",
    "\n",
    "def recreateMetadata(datasetId, metadata, layerId=None, widgetId=None, toEnv = 'prod'):\n",
    "    '''\n",
    "    Copy the metadata from one env to the other\n",
    "    '''\n",
    "    \n",
    "    serverUrl = {\n",
    "        'prod': prod_server,\n",
    "        'staging': staging_server\n",
    "    }\n",
    "    headers = setTokenHeader(toEnv)\n",
    "    \n",
    "    if metadata.get('type')!='metadata':\n",
    "        return None\n",
    "    if layerId and widgetId:\n",
    "        raise Exception(\"layerId and widgetId not allowed at the same time\")\n",
    "    elif layerId:\n",
    "        url = f'{serverUrl[toEnv]}/v1/dataset/{datasetId}/layer/{layerId}/metadata'\n",
    "    elif widgetId:\n",
    "        url = f'{serverUrl[toEnv]}/v1/dataset/{datasetId}/widget/{widgetId}/metadata'\n",
    "    else:\n",
    "        url = f'{serverUrl[toEnv]}/v1/dataset/{datasetId}/metadata'\n",
    "    \n",
    "    body = copyAssetBody(metadata.get('attributes'))\n",
    "    \n",
    "    try:\n",
    "        response = upsert()\n",
    "        return response(url, body, headers)\n",
    "    except Exception as e:\n",
    "        print('Post operation was not succesfull, trying to update instead')\n",
    "        response = upsert(True)\n",
    "        return response(url, body, headers)\n",
    "        pass\n",
    "\n",
    "def recreateVocabulary(datasetId, vocabulary, toEnv = 'prod'):\n",
    "    '''\n",
    "    Copy the vocabulary from one env to the other\n",
    "    '''\n",
    "    \n",
    "    serverUrl = {\n",
    "        'prod': prod_server,\n",
    "        'staging': staging_server\n",
    "    }\n",
    "    \n",
    "    if vocabulary.get('type')!='vocabulary':\n",
    "        return None\n",
    "    \n",
    "    headers = setTokenHeader(toEnv)\n",
    "    \n",
    "    url = f\"{serverUrl[toEnv]}/v1/dataset/{datasetId}/vocabulary/{vocabulary['attributes']['name']}\"\n",
    "    body = {\n",
    "        'application': vocabulary['attributes'].get('application'),\n",
    "        'tags': vocabulary['attributes'].get('tags')\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = upsert()\n",
    "        return response(url, body, headers)\n",
    "    except Exception as e:\n",
    "        print(f'{bcolors.WARNING}Post operation was not succesfull, trying to update instead{bcolors.ENDC}')\n",
    "        response = upsert(True)\n",
    "        return response(url, body, headers)\n",
    "        pass\n",
    "\n",
    "def getAssetList(fromEnv = 'prod', datasetList=None):\n",
    "    '''\n",
    "    Gets a list of assets from the selected env or from the constrained dataset list\n",
    "    '''\n",
    "    serverUrl = {\n",
    "        'prod': prod_server,\n",
    "        'staging': staging_server\n",
    "    }\n",
    "    headers = setTokenHeader(fromEnv)\n",
    "    url = f'{serverUrl[fromEnv]}/v1/dataset'\n",
    "    payload={\n",
    "        'application':'rw',\n",
    "        'status':'saved',\n",
    "        'includes':'widget,layer,vocabulary,metadata',\n",
    "        'page[size]':1613982331640\n",
    "    }\n",
    "    if datasetList:\n",
    "        url = f'{serverUrl[fromEnv]}/v1/dataset/find-by-ids'\n",
    "        body = {\n",
    "            'ids': datasetList\n",
    "        }\n",
    "        return postAssets(url, body, headers, payload)\n",
    "    else:\n",
    "        return getAssets(url, payload)\n",
    "    \n",
    "def backupAssets(env = 'prod', datasetList = None):\n",
    "    '''\n",
    "    save a backup of production data just in case we need to recreate it again\n",
    "    '''\n",
    "    data = getAssetList(env, datasetList)\n",
    "    \n",
    "\n",
    "    with open(f'RW_{Env}_backup_{datetime.now().strftime(\"%Y%m%d-%H%M%S\")}.json', 'w') as outfile:\n",
    "        json.dump(data, outfile)\n",
    "\n",
    "def deleteDataFrom(env='staging', datasetList = None):\n",
    "    '''\n",
    "    Deletes all assets from an env.\n",
    "    '''\n",
    "    serverUrl = {\n",
    "        'prod': prod_server,\n",
    "        'staging': staging_server\n",
    "    }\n",
    "    userConfirmation = input(f'{bcolors.WARNING}Are you sure you want to delete \\\n",
    "        {str(datasetList)  if datasetList else \"everything\" } in {env}:{bcolors.ENDC} \\\n",
    "        Y/n') or \"N\"\n",
    "    if userConfirmation == 'Y':\n",
    "        headers = setTokenHeader(env)\n",
    "        data = getAssetList(env, datasetList)\n",
    "        \n",
    "        for dataset in data['data']:\n",
    "            #@TODO: this needs to be reworked a bit\n",
    "            try:\n",
    "                logger.info(f\"deleting {serverUrl[env]}/v1/dataset/{dataset['id']}... \")\n",
    "                status = deleteAssets(f\"{serverUrl[env]}/v1/dataset/{dataset['id']}\", headers)\n",
    "                    \n",
    "            except re.exceptions.HTTPError as err:\n",
    "                logger.error(err)\n",
    "                pass\n",
    "    else:\n",
    "        print('nothing was deleted')\n",
    "\n",
    "def assetIdToBeSync(sync, syncList, assetToSync, fromEnv, toEnv):\n",
    "    '''\n",
    "    controls the asset id to be sync\n",
    "    '''\n",
    "    if sync:\n",
    "        assetId = False\n",
    "        for asset in syncList:\n",
    "            if asset.get('type') == assetToSync.get('type') and asset.get(f'{fromEnv}Id') == assetToSync.get('id'):\n",
    "                assetId = asset.get(f'{toEnv}Id')\n",
    "        return assetId\n",
    "\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def copyAssets(assetList, sync=False, fromEnv='prod', toEnv='staging'):\n",
    "    '''\n",
    "    Creates a new copy or syncs the assets that we set up in the fromEnv into the destination Env \n",
    "    '''\n",
    "    if fromEnv == toEnv:\n",
    "        raise NameError(f'fromEnv:{fromEnv} and toEnv:{toEnv} cannot be the same')\n",
    "        \n",
    "    if not assetList or len(assetList) == 0:\n",
    "        raise IndexError(f'asset list is empty or not defined')\n",
    "        \n",
    "    \n",
    "    dataAssets = []    \n",
    "    \n",
    "    if sync:\n",
    "        newDatasetList = [asset[f'{fromEnv}Id'] for asset in assetList if asset['type'] == 'dataset']\n",
    "        dataAssets = getAssetList(fromEnv, newDatasetList)\n",
    "\n",
    "    else:   \n",
    "        dataAssets = getAssetList(fromEnv, assetList)\n",
    "    \n",
    "    try:\n",
    "        print(f'{bcolors.OKBLUE}Preparing to {\"sync\" if sync else \"copy\"} from {fromEnv} to {toEnv}...{bcolors.ENDC}')\n",
    "        resources = []\n",
    "        \n",
    "        # @TODO:\n",
    "        # Improve loop performance with multiprocessing\n",
    "        # move loops into reusable function based on type\n",
    "        # For sync only path updated data\n",
    "        \n",
    "        for dataset in dataAssets['data']:\n",
    "            toDatasetId = assetIdToBeSync(sync, assetList, dataset, fromEnv, toEnv)\n",
    "            if toDatasetId:\n",
    "                logger.info(f'sync [{fromEnv}]dataset: {dataset.get(\"id\")}')\n",
    "                logger.info(f'with [{toEnv}]dataset: {toDatasetId}')\n",
    "            newDataset = recreateDataset(dataset, toEnv, toDatasetId)\n",
    "\n",
    "            resources.append({\n",
    "                'type': 'dataset',\n",
    "                f'{fromEnv}Id':dataset.get('id'),\n",
    "                f'{toEnv}Id': newDataset['data'].get('id')\n",
    "            })\n",
    "\n",
    "            for vocabulary in dataset['attributes'].get('vocabulary'):\n",
    "                newVocabulary = recreateVocabulary(newDataset['data'].get('id'), vocabulary, toEnv)\n",
    "                \n",
    "                resources.append({\n",
    "                'type': 'vocabulary',\n",
    "                f'{fromEnv}Id':vocabulary.get('id'),\n",
    "                f'{toEnv}Id': newVocabulary['data']\n",
    "            })\n",
    "\n",
    "            # sync layers\n",
    "            for layer in dataset['attributes'].get('layer'):\n",
    "                \n",
    "                toLayerId = assetIdToBeSync(sync, assetList, layer, fromEnv, toEnv)\n",
    "                if toLayerId:\n",
    "                    logger.info(f'sync [{fromEnv}]layer: {layer.get(\"id\")}')\n",
    "                    logger.info(f'with [{toEnv}]layer: {toLayerId}')\n",
    "                \n",
    "                newLayer = recreateLayer(newDataset['data'].get('id'), layer, toEnv, toLayerId)\n",
    "                \n",
    "                resources.append({\n",
    "                'type': 'layer',\n",
    "                f'{fromEnv}Id':layer.get('id'),\n",
    "                f'{toEnv}Id': newLayer['data'].get('id')\n",
    "                })\n",
    "                \n",
    "                fromLayerMetadata = getSubAssetMetadata(dataset.get(\"id\"), layerId=layer.get(\"id\"), fromEnv=fromEnv)\n",
    "                \n",
    "                if fromLayerMetadata:\n",
    "                    for layerMetadata in fromLayerMetadata.get('data', {}):\n",
    "                        logger.info('creating metadata for layer...')\n",
    "                        newMetadata = recreateMetadata(newDataset['data'].get('id'), layerMetadata, layerId=newLayer['data'].get('id'), toEnv=toEnv)\n",
    "                        \n",
    "                        resources.append({\n",
    "                        'type': 'metadata',\n",
    "                        f'{fromEnv}Id':layerMetadata.get('id'),\n",
    "                        f'{toEnv}Id': newMetadata['data']\n",
    "                        })   \n",
    "            \n",
    "            # remove toEnv layers that are not on fromEnv            \n",
    "            for layer in getAssetList(toEnv, [toDatasetId])['data'][0]['attributes'].get('layer'):\n",
    "                if layer.get(\"id\") not in [asset[f'{toEnv}Id'] for asset in resources if asset['type'] == 'layer']:\n",
    "                    headers = setTokenHeader(toEnv)\n",
    "                    serverUrl = {\n",
    "                        'prod': prod_server,\n",
    "                        'staging': staging_server\n",
    "                    }\n",
    "                    url = f'{serverUrl[toEnv]}/v1/dataset/{toDatasetId}/layer/{layer.get(\"id\")}'\n",
    "                    deleteAssets(url, headers)   \n",
    "            \n",
    "            # sync widgets\n",
    "            for widget in dataset['attributes'].get('widget'):\n",
    "                \n",
    "                toWidgetId = assetIdToBeSync(sync, assetList, widget, fromEnv, toEnv)\n",
    "                if toWidgetId:\n",
    "                    logger.info(f'sync [{fromEnv}]widget: {widget.get(\"id\")}')\n",
    "                    logger.info(f'with [{toEnv}]widget: {toWidgetId}')\n",
    "                    \n",
    "                newWidget = recreateWidget(newDataset['data'].get('id'), widget, toEnv, toWidgetId)                \n",
    "                resources.append({\n",
    "                'type': 'widget',\n",
    "                f'{fromEnv}Id':widget.get('id'),\n",
    "                f'{toEnv}Id': newWidget['data'].get('id')\n",
    "                })\n",
    "                \n",
    "                fromWidgetMetadata = getSubAssetMetadata(dataset.get(\"id\"), widgetId=widget.get(\"id\"), fromEnv=fromEnv)\n",
    "                \n",
    "                if fromWidgetMetadata:\n",
    "                    for widgetMetadata in fromWidgetMetadata.get('data', {}):\n",
    "                        logger.info('creating metadata for widget...')\n",
    "                        newMetadata = recreateMetadata(newDataset['data'].get('id'), widgetMetadata, widgetId=newWidget['data'].get('id'), toEnv=toEnv)\n",
    "                        \n",
    "                        resources.append({\n",
    "                        'type': 'metadata',\n",
    "                        f'{fromEnv}Id':widgetMetadata.get('id'),\n",
    "                        f'{toEnv}Id': newMetadata['data']\n",
    "                        })\n",
    "\n",
    "            # remove toEnv widgets that are not on fromEnv            \n",
    "            for widget in getAssetList(toEnv, [toDatasetId])['data'][0]['attributes'].get('widget'):\n",
    "                if widget.get(\"id\") not in [asset[f'{toEnv}Id'] for asset in resources if asset['type'] == 'widget']:\n",
    "                    headers = setTokenHeader(toEnv)\n",
    "                    serverUrl = {\n",
    "                        'prod': prod_server,\n",
    "                        'staging': staging_server\n",
    "                    }\n",
    "                    url = f'{serverUrl[toEnv]}/v1/dataset/{toDatasetId}/widget/{widget.get(\"id\")}'\n",
    "                    deleteAssets(url, headers)       \n",
    "\n",
    "            for metadata in dataset['attributes'].get('metadata'):\n",
    "                logger.info('creating metadata')\n",
    "                newMetadata = recreateMetadata(newDataset['data'].get('id'), metadata, toEnv=toEnv)\n",
    "                \n",
    "                resources.append({\n",
    "                'type': 'metadata',\n",
    "                f'{fromEnv}Id':metadata.get('id'),\n",
    "                f'{toEnv}Id': newMetadata['data']\n",
    "                })\n",
    "    except NameError or IndexError as e:\n",
    "        logger.error(e)\n",
    "        raise e\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    print(f'{bcolors.OKGREEN}{\"sync\" if sync else \"copy\"} process finished{bcolors.ENDC}')\n",
    "    \n",
    "    if not sync and len(resources) > 0:\n",
    "        filename = f'RW_prod_staging_match_{datetime.now().strftime(\"%Y%m%d-%H%M%S\")}.json'\n",
    "        print(f'creating sync file with name: {filename}')\n",
    "        with open(filename, 'w') as outfile:\n",
    "            json.dump(resources, outfile)\n",
    "        \n",
    "        return filename\n",
    "\n",
    "    if sync and len(resources) > 0:\n",
    "        filename = f'RW_prod_staging_match_{datetime.now().strftime(\"%Y%m%d-%H%M%S\")}.json'\n",
    "        print(f'replacing sync file {syncFile} with {filename}')\n",
    "        with open(filename, 'w') as outfile:\n",
    "            json.dump(resources, outfile)\n",
    "        os.remove(syncFile)\n",
    "        \n",
    "        return filename\n",
    "    \n",
    "    \n",
    "        \n",
    "def syncAssets(syncList, fromEnv='prod', toEnv='staging'):\n",
    "    '''\n",
    "    Allows sync of Assets\n",
    "    '''\n",
    "    \n",
    "    return copyAssets(syncList, True, fromEnv, toEnv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing\n",
    "## Get list of assets that we want to modify or sync"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List of assets:\n",
    "\n",
    "* `datasetsProd` will contain the id of the assets in productioon that need to be migrated to `staging`. We need to make sure that this list is in sync with the document we have shared with the assets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For testing purposes\n",
    "Dummy assests to create assets in production environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': {'id': '6a3aa408-b3d3-44c6-89b7-93fbfa545489',\n",
       "  'type': 'dataset',\n",
       "  'attributes': {'name': 'This is a test',\n",
       "   'slug': 'This-is-a-test_9',\n",
       "   'type': None,\n",
       "   'subtitle': None,\n",
       "   'application': ['rw'],\n",
       "   'dataPath': None,\n",
       "   'attributesPath': None,\n",
       "   'connectorType': 'rest',\n",
       "   'provider': 'cartodb',\n",
       "   'userId': '57a0aa1071e394dd32ffe137',\n",
       "   'connectorUrl': 'https://wri-rw.carto.com/api/v2/sql?q=select * from air_temo_anomalies',\n",
       "   'sources': [],\n",
       "   'tableName': 'air_temo_anomalies',\n",
       "   'status': 'pending',\n",
       "   'published': False,\n",
       "   'overwrite': False,\n",
       "   'mainDateField': None,\n",
       "   'env': 'production',\n",
       "   'geoInfo': False,\n",
       "   'protected': False,\n",
       "   'legend': {'date': [],\n",
       "    'region': [],\n",
       "    'country': [],\n",
       "    'nested': [],\n",
       "    'integer': [],\n",
       "    'short': [],\n",
       "    'byte': [],\n",
       "    'double': [],\n",
       "    'float': [],\n",
       "    'half_float': [],\n",
       "    'scaled_float': [],\n",
       "    'boolean': [],\n",
       "    'binary': [],\n",
       "    'text': [],\n",
       "    'keyword': []},\n",
       "   'clonedHost': {},\n",
       "   'errorMessage': None,\n",
       "   'taskId': None,\n",
       "   'createdAt': '2021-06-07T09:36:12.332Z',\n",
       "   'updatedAt': '2021-06-07T09:36:12.332Z',\n",
       "   'dataLastUpdated': None,\n",
       "   'widgetRelevantProps': [],\n",
       "   'layerRelevantProps': []}}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dummy data to test the notebook: creation of a dummy dataset with a layer in production.\n",
    "toEnv = 'prod'\n",
    "serverUrl = {\n",
    "        'prod': prod_server,\n",
    "        'staging': staging_server\n",
    "    }\n",
    "headers = setTokenHeader(toEnv)\n",
    "urlDataset = f'{serverUrl[toEnv]}/v1/dataset'\n",
    "bodyDataset = {'dataset':{\n",
    "    'application': ['rw'],\n",
    "    'name': 'This is a test',\n",
    "    'connectorType': 'rest',\n",
    "    'provider': 'cartodb',\n",
    "    'published': False,\n",
    "    'overwrite': False,\n",
    "    'protected':False,\n",
    "    'env': 'production',\n",
    "    'connectorUrl': \"https://wri-rw.carto.com/api/v2/sql?q=select * from air_temo_anomalies\"\n",
    "    }\n",
    "}\n",
    "\n",
    "responseDataset = postAssets(urlDataset, bodyDataset, headers)\n",
    "responseDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': {'id': 'c21dd7ab-e729-4811-9433-8333b1d7c9e9',\n",
       "  'type': 'layer',\n",
       "  'attributes': {'name': 'test-121',\n",
       "   'slug': 'test-121_2',\n",
       "   'dataset': '6a3aa408-b3d3-44c6-89b7-93fbfa545489',\n",
       "   'application': ['rw'],\n",
       "   'iso': [],\n",
       "   'provider': 'cartodb',\n",
       "   'userId': '57a0aa1071e394dd32ffe137',\n",
       "   'default': True,\n",
       "   'protected': False,\n",
       "   'published': False,\n",
       "   'env': 'production',\n",
       "   'layerConfig': {'body': {}},\n",
       "   'legendConfig': {},\n",
       "   'interactionConfig': {},\n",
       "   'applicationConfig': {},\n",
       "   'staticImageConfig': {},\n",
       "   'createdAt': '2021-06-07T09:36:15.327Z',\n",
       "   'updatedAt': '2021-06-07T09:36:15.327Z'}}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urlLayer = f'{urlDataset}/{responseDataset[\"data\"].get(\"id\")}/layer'\n",
    "bodyLayer = {\n",
    "        'application': ['rw'],\n",
    "        'name': 'test-121',\n",
    "        'provider': 'cartodb',\n",
    "        'default': True,\n",
    "        'published': False,\n",
    "        'env': 'production',\n",
    "        'layerConfig': {\n",
    "            \"body\": {}\n",
    "            },\n",
    "        'legendConfig': {},\n",
    "        'interactionConfig': {},\n",
    "        'applicationConfig': {}\n",
    "    }\n",
    "responseLayer = postAssets(urlLayer, bodyLayer, headers)\n",
    "responseLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': {'id': '5f169df0-a293-4588-bbcd-521ee9484cd6',\n",
       "  'type': 'widget',\n",
       "  'attributes': {'name': 'test-121',\n",
       "   'dataset': '6a3aa408-b3d3-44c6-89b7-93fbfa545489',\n",
       "   'slug': 'test-121_2',\n",
       "   'userId': '57a0aa1071e394dd32ffe137',\n",
       "   'application': ['rw'],\n",
       "   'verified': False,\n",
       "   'default': True,\n",
       "   'protected': False,\n",
       "   'defaultEditableWidget': False,\n",
       "   'published': False,\n",
       "   'freeze': False,\n",
       "   'env': 'production',\n",
       "   'widgetConfig': {'body': {}},\n",
       "   'template': False,\n",
       "   'createdAt': '2021-06-07T09:36:17.153Z',\n",
       "   'updatedAt': '2021-06-07T09:36:17.154Z'}}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urlWidget = f'{urlDataset}/{responseDataset[\"data\"].get(\"id\")}/widget'\n",
    "bodyWidget = {\n",
    "        'application': ['rw'],\n",
    "        'name': 'test-121',\n",
    "        'default': True,\n",
    "        'published': False,\n",
    "        'env': 'production',\n",
    "        'widgetConfig': {\n",
    "            \"body\": {}\n",
    "            }\n",
    "    }\n",
    "responseWidget = postAssets(urlWidget, bodyWidget, headers)\n",
    "responseWidget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': [{'id': 'knowledge_graph',\n",
       "   'type': 'vocabulary',\n",
       "   'attributes': {'tags': ['geospatial'],\n",
       "    'name': 'knowledge_graph',\n",
       "    'application': 'rw'}}]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urlVocabulary = f'{urlDataset}/{responseDataset[\"data\"].get(\"id\")}/vocabulary/knowledge_graph'\n",
    "bodyVocabulary = {\n",
    "        'application': 'rw',\n",
    "        'tags':[\"geospatial\"]\n",
    "    }\n",
    "responseVocabulary = postAssets(urlVocabulary, bodyVocabulary, headers)\n",
    "responseVocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': [{'id': '60bde8962852be001ba7e42b',\n",
       "   'type': 'metadata',\n",
       "   'attributes': {'dataset': '6a3aa408-b3d3-44c6-89b7-93fbfa545489',\n",
       "    'application': 'rw',\n",
       "    'resource': {'id': '6a3aa408-b3d3-44c6-89b7-93fbfa545489',\n",
       "     'type': 'dataset'},\n",
       "    'language': 'eng',\n",
       "    'name': 'this is a dummy dataset',\n",
       "    'description': 'Lorem Ipsum',\n",
       "    'createdAt': '2021-06-07T09:36:22.304Z',\n",
       "    'updatedAt': '2021-06-07T09:36:22.304Z',\n",
       "    'status': 'published'}}]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urlMetadataDataset = f'{urlDataset}/{responseDataset[\"data\"].get(\"id\")}/metadata'\n",
    "bodyMetadataDataset = {\n",
    "        'application': 'rw',\n",
    "        'language':'ENG',\n",
    "        'name':'this is a dummy dataset',\n",
    "        'description':'Lorem Ipsum'\n",
    "    }\n",
    "responseMetadataDataset = postAssets(urlMetadataDataset, bodyMetadataDataset, headers)\n",
    "responseMetadataDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urlMetadataLayer = f'{urlDataset}/{responseDataset[\"data\"].get(\"id\")}/layer/{responseLayer[\"data\"].get(\"id\")}/metadata'\n",
    "bodyMetadataLayer = {\n",
    "        'application': 'rw',\n",
    "        'language':'ENG',\n",
    "        'name':'this is a dummy Layer',\n",
    "        'description':'Lorem Ipsum'\n",
    "    }\n",
    "responseMetadataLayer = postAssets(urlMetadataLayer, bodyMetadataLayer, headers)\n",
    "responseMetadataLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': [{'id': '60bde89a3cc064001b3675b9',\n",
       "   'type': 'metadata',\n",
       "   'attributes': {'dataset': '6a3aa408-b3d3-44c6-89b7-93fbfa545489',\n",
       "    'application': 'rw',\n",
       "    'resource': {'id': '5f169df0-a293-4588-bbcd-521ee9484cd6',\n",
       "     'type': 'widget'},\n",
       "    'language': 'eng',\n",
       "    'name': 'this is a dummy widget',\n",
       "    'description': 'Lorem Ipsum',\n",
       "    'createdAt': '2021-06-07T09:36:26.194Z',\n",
       "    'updatedAt': '2021-06-07T09:36:26.194Z',\n",
       "    'status': 'published'}}]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urlMetadatawidget = f'{urlDataset}/{responseDataset[\"data\"].get(\"id\")}/widget/{responseWidget[\"data\"].get(\"id\")}/metadata'\n",
    "bodyMetadatawidget = {\n",
    "        'application': 'rw',\n",
    "        'language':'ENG',\n",
    "        'name':'this is a dummy widget',\n",
    "        'description':'Lorem Ipsum'\n",
    "    }\n",
    "responseMetadatawidget = postAssets(urlMetadatawidget, bodyMetadatawidget, headers)\n",
    "responseMetadatawidget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List of assets:\n",
    "\n",
    "* we need to make sure that this list is in sync with the document we have shared with the assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['6a3aa408-b3d3-44c6-89b7-93fbfa545489']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# in the future we can automate this listing based on the doc using the google sheet api both for writing and reading from\n",
    "# providing a sample of the list by printing it\n",
    "datasetsProd = [responseDataset['data']['id']]\n",
    "datasetsProd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backup Data in both environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#backupAssets('prod')\n",
    "#backupAssets('staging')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only do this if you want to clean data in staging. \n",
    "* You will need to be logged in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#deleteDataFrom()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy resources from production to staging. \n",
    "The running time will depend on the size of the asset.   \n",
    "Running this cell is only needed to create new assets from `production` to `staging`.\n",
    "A json file is created with a unique name in local. The json files contains for each assest:\n",
    "- type: this can be a \"layer\", a \"dataset\", a \"widget\", \"vocabulary\", \"metadata\"\n",
    "- prodId: the id of the item in `production`\n",
    "- stagingId: the id of the item in `staging`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[94mPreparing to copy from prod to staging...\u001b[0m\n",
      "INFO:root:creating metadata for widget...\n",
      "INFO:root:creating metadata\n",
      "\u001b[92mcopy process finished\u001b[0m\n",
      "creating sync file with name: RW_prod_staging_match_20210617-152358.json\n"
     ]
    }
   ],
   "source": [
    "syncFile = copyAssets(datasetsProd, False, fromEnv='prod', toEnv= 'staging')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open sync list of assets, match items with list and update them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:root:sync [prod]dataset: b2483333-693a-44e2-ae00-47f21c6a00bd\n",
      "INFO:root:with [staging]dataset: e8c1e282-961a-436f-9147-99abca05c09e\n",
      "\u001b[94mPreparing to sync from prod to staging...\u001b[0m\n",
      "ERROR:root:response: \n",
      "ERROR:root:<Response [400]>\n",
      "ERROR:root:url: \n",
      "ERROR:root:https://staging-api.resourcewatch.org/v1/dataset/e8c1e282-961a-436f-9147-99abca05c09e/vocabulary/knowledge_graph\n",
      "ERROR:root:body: \n",
      "ERROR:root:{\"application\": \"rw\", \"tags\": [\"geospatial\", \"table\", \"global\", \"school\", \"annual\", \"education\", \"historical\", \"vector\", \"society\"]}\n",
      "\u001b[93mPost operation was not succesfull, trying to update instead\u001b[0m\n",
      "INFO:root:sync [prod]layer: 42f57f77-432d-4149-a04c-29e4114d93aa\n",
      "INFO:root:with [staging]layer: 5a508717-9e3d-4946-bde3-041b256a34b2\n",
      "INFO:root:sync [prod]layer: c283cbb6-2cc1-4662-bd66-4c175397c963\n",
      "INFO:root:with [staging]layer: c07bb2cd-0f43-4201-b9ba-8884b18e8e64\n",
      "INFO:root:sync [prod]layer: 8857f26f-e606-41f0-bdf9-03122679119f\n",
      "INFO:root:with [staging]layer: b7706f25-e39f-4e84-ad6e-9c8eefbf143e\n",
      "INFO:root:sync [prod]layer: 9b5563e4-6b05-43c3-95ba-30bbe89f4fa0\n",
      "INFO:root:with [staging]layer: 42807462-399a-43ee-9a81-11baf1b6b1ce\n",
      "INFO:root:sync [prod]layer: 035868e0-2e23-4868-9b31-3bcbe1c74c87\n",
      "INFO:root:with [staging]layer: d4816873-1a7f-4282-9b84-76908479cf72\n",
      "INFO:root:sync [prod]layer: 44cf4889-04cb-4da9-9c60-669470914ff6\n",
      "INFO:root:with [staging]layer: 7f9b0c65-3337-40a5-a368-5279065fc630\n",
      "INFO:root:sync [prod]widget: 318a08d0-44d7-4bff-a549-15e06f8079f0\n",
      "INFO:root:with [staging]widget: b862b6e4-41d6-4fb1-a03b-6b6ef5c0897a\n",
      "INFO:root:creating metadata for widget...\n",
      "ERROR:root:response: \n",
      "ERROR:root:<Response [400]>\n",
      "ERROR:root:url: \n",
      "ERROR:root:https://staging-api.resourcewatch.org/v1/dataset/e8c1e282-961a-436f-9147-99abca05c09e/widget/b862b6e4-41d6-4fb1-a03b-6b6ef5c0897a/metadata\n",
      "ERROR:root:body: \n",
      "ERROR:root:{\"application\": \"rw\", \"resource\": {\"id\": \"318a08d0-44d7-4bff-a549-15e06f8079f0\", \"type\": \"widget\"}, \"language\": \"en\", \"info\": {\"caption\": \"\"}}\n",
      "Post operation was not succesfull, trying to update instead\n",
      "INFO:root:creating metadata\n",
      "ERROR:root:response: \n",
      "ERROR:root:<Response [400]>\n",
      "ERROR:root:url: \n",
      "ERROR:root:https://staging-api.resourcewatch.org/v1/dataset/e8c1e282-961a-436f-9147-99abca05c09e/metadata\n",
      "ERROR:root:body: \n",
      "ERROR:root:{\"application\": \"rw\", \"resource\": {\"id\": \"b2483333-693a-44e2-ae00-47f21c6a00bd\", \"type\": \"dataset\"}, \"language\": \"en\", \"name\": \"Out-Of-School Rate\", \"description\": \"The rate of out-of-school children, generated by the UNESCO Institute for Statistics (UIS), is the percentage of children not in school by year per country. The data extend back to 2014. The UIS gathers these statistics using questionnaires, which are sent to UNESCO Member States annually. The questionnaires are based on international standards, classifications, and measures that are regularly reviewed and modified by the UIS in order to address emerging statistical issues and improve the quality of data. The UIS collaborates with the Organisation for Economic Co-operation and Development (OECD) and Eurostat to collect data with surveys (collectively UOE). The UOE questionnaire compiles data from high- and middle-income countries that are members or partner countries of the OECD or Eurostat. The UOE survey gathers more detailed education statistics. Midyear population estimates for countries with at least 90,000 inhabitants are provided by the UN Population Division to calculate the percentages. Resource Watch shows only a subset of the data set. For access to the full data set and additional information, see the Learn More link.  \\n  \\n### Additional Information  \\n  \\nResource Watch shows only a subset of the dataset. For access to the full dataset and additional information, click on the \\u201cLearn more\\u201d button.  \\n  \\n### Disclaimer  \\n  \\nExcerpts of this description page were taken from the source metadata.\", \"source\": \"UNESCO IS\", \"info\": {\"rwId\": \"soc.039.rw1\", \"data_type\": \"Tabular\", \"name\": \"Out-Of-School Rate\", \"sources\": [{\"source-name\": \"\", \"id\": 0, \"source-description\": \"United Nations Educational, Scientific and Cultural Organization Institute for Statistics (UNESCO IS)\"}], \"technical_title\": \"Out-of-school Rate for Children, Adolescents and Youth of Primary, Lower Secondary and Upper Secondary School Age\", \"functions\": \"Proportion of children, adolescents and youth of primary, lower secondary and upper secondary school age of both sexes who are out of school\", \"cautions\": \"- Inconsistencies between enrolment and population data from different sources may result in inaccurate estimates of out-of-school children and adolescents.\\n- For primary, lower secondary and upper secondary education, the official age groups for the respective level of education are used in the indicator calculation.\", \"citation\": \"United Nations Educational, Scientific and Cultural Organization Institute for Statistics. 2021. \\\"Sustainable Development Goals 1 and 4: 4.1.4 Out-of-school rate by school age and sex (administrative data)\\\" Accessed March 24, 2021. Accessed through Resource Watch, (date). [www.resourcewatch.org](https://www.resourcewatch.org).\", \"license\": \"Restrictions apply\", \"license_link\": \"http://uis.unesco.org/en/terms-and-conditions\", \"geographic_coverage\": \"Global\", \"spatial_resolution\": \"National\", \"date_of_content\": \"2014-2019\", \"frequency_of_updates\": \"Annual\", \"learn_more_link\": \"http://data.uis.unesco.org/index.aspx\", \"data_download_link\": \"https://wri-public-data.s3.amazonaws.com/resourcewatch/soc_039_rw1_out_of_school_rate.zip\", \"data_download_original_link\": \"http://data.uis.unesco.org/index.aspx\"}, \"columns\": {\"country\": {\"alias\": \"Country\"}, \"yr_1999\": {\"description\": \"Percentage out-of-school rate of children of primary school age, both sexes\", \"alias\": \"1999\"}, \"yr_2000\": {\"description\": \"Percentage out-of-school rate of children of primary school age, both sexes\", \"alias\": \"2000\"}, \"yr_2001\": {\"description\": \"Percentage out-of-school rate of children of primary school age, both sexes\", \"alias\": \"2001\"}, \"yr_2002\": {\"description\": \"Percentage out-of-school rate of children of primary school age, both sexes\", \"alias\": \"2002\"}, \"yr_2003\": {\"description\": \"Percentage out-of-school rate of children of primary school age, both sexes\", \"alias\": \"2003\"}, \"yr_2004\": {\"description\": \"Percentage out-of-school rate of children of primary school age, both sexes\", \"alias\": \"2004\"}, \"yr_2005\": {\"description\": \"Percentage out-of-school rate of children of primary school age, both sexes\", \"alias\": \"2005\"}, \"yr_2006\": {\"description\": \"Percentage out-of-school rate of children of primary school age, both sexes\", \"alias\": \"2006\"}, \"yr_2007\": {\"description\": \"Percentage out-of-school rate of children of primary school age, both sexes\", \"alias\": \"2007\"}, \"yr_2008\": {\"description\": \"Percentage out-of-school rate of children of primary school age, both sexes\", \"alias\": \"2008\"}, \"yr_2009\": {\"description\": \"Percentage out-of-school rate of children of primary school age, both sexes\", \"alias\": \"2009\"}, \"yr_2010\": {\"description\": \"Percentage out-of-school rate of children of primary school age, both sexes\", \"alias\": \"2010\"}, \"yr_2011\": {\"description\": \"Percentage out-of-school rate of children of primary school age, both sexes\", \"alias\": \"2011\"}, \"yr_2012\": {\"description\": \"Percentage out-of-school rate of children of primary school age, both sexes\", \"alias\": \"2012\"}, \"yr_2013\": {\"description\": \"Percentage out-of-school rate of children of primary school age, both sexes\", \"alias\": \"2013\"}, \"yr_2014\": {\"description\": \"Percentage out-of-school rate of children of primary school age, both sexes\", \"alias\": \"2014\"}, \"yr_2015\": {\"description\": \"Percentage out-of-school rate of children of primary school age, both sexes\", \"alias\": \"2015\"}, \"yr_2016\": {\"description\": \"Percentage out-of-school rate of children of primary school age, both sexes\", \"alias\": \"2016\"}, \"datetime\": {\"alias\": \"Year\", \"description\": \"This dataset is an annual dataset. Timestamps were created using the first day of each year. (ex: 2020 data would be represented as January 1, 2020)\"}, \"rw_country_code\": {\"alias\": \"ISO Code\"}, \"yr_data\": {\"alias\": \"Out-of-School Rate (%)\", \"description\": \"Proportion of children, adolescents, and youth of primary and secondary school age of both sexes who are out of school\"}, \"rw_country_name\": {\"alias\": \"Country\"}, \"location\": {\"alias\": \"Country Code\"}, \"value\": {\"alias\": \"Out-of-school Rate (%)\", \"description\": \"Proportion of children, adolescents and youth of primary, lower secondary and upper secondary school age of both sexes who are out of school.\"}, \"flags\": {\"alias\": \"Flags\", \"description\": \"National Estimation/UIS Estimation\"}}}\n",
      "Post operation was not succesfull, trying to update instead\n",
      "\u001b[92msync process finished\u001b[0m\n",
      "replacing sync file RW_prod_staging_match_20210621-160916.json with RW_prod_staging_match_20210621-161100.json\n"
     ]
    }
   ],
   "source": [
    "# use the printed json filename in the previous cell\n",
    "with open(syncFile) as json_file:\n",
    "    syncList = json.load(json_file)\n",
    "\n",
    "syncFile = syncAssets(syncList, fromEnv='prod', toEnv='staging')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\u001b[93mAre you sure you want to delete         ['6a3aa408-b3d3-44c6-89b7-93fbfa545489'] in prod:\u001b[0m         Y/n Y\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:deleting https://api.resourcewatch.org/v1/dataset/6a3aa408-b3d3-44c6-89b7-93fbfa545489... \n"
     ]
    }
   ],
   "source": [
    "# delete testing datasets from both envs after testing:\n",
    "deleteDataFrom('prod', [responseDataset['data']['id']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\u001b[93mAre you sure you want to delete         ['41e6b9c7-481c-403c-a609-82adc3e59000'] in staging:\u001b[0m         Y/n Y\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:deleting https://staging-api.globalforestwatch.org/v1/dataset/41e6b9c7-481c-403c-a609-82adc3e59000... \n"
     ]
    }
   ],
   "source": [
    "deleteDataFrom('staging', [syncList[0]['stagingId']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.7 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "interpreter": {
   "hash": "1936b053440c27f530542f53326030d97194af8aa0e5ac751988556632f9c990"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}